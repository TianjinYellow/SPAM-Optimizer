2024-12-25 06:47:08.091 | INFO     | __main__:main:217 - Using dist with rank 0 (only rank 0 will log)
2024-12-25 06:47:08.092 | INFO     | __main__:main:218 - ****************************************
2024-12-25 06:47:08.092 | INFO     | __main__:main:219 - Starting training with the arguments
2024-12-25 06:47:08.092 | INFO     | __main__:main:221 - model_config                   configs/llama_60m.json
model_config                   configs/llama_60m.json
2024-12-25 06:47:08.092 | INFO     | __main__:main:221 - use_hf_model                   False
use_hf_model                   False
2024-12-25 06:47:08.092 | INFO     | __main__:main:221 - continue_from                  None
continue_from                  None
2024-12-25 06:47:08.092 | INFO     | __main__:main:221 - batch_size                     128
batch_size                     128
2024-12-25 06:47:08.092 | INFO     | __main__:main:221 - gradient_accumulation          4
gradient_accumulation          4
2024-12-25 06:47:08.093 | INFO     | __main__:main:221 - total_batch_size               512
total_batch_size               512
2024-12-25 06:47:08.093 | INFO     | __main__:main:221 - max_length                     256
max_length                     256
2024-12-25 06:47:08.093 | INFO     | __main__:main:221 - optimizer                      SPAM
optimizer                      SPAM
2024-12-25 06:47:08.093 | INFO     | __main__:main:221 - lr                             0.004
lr                             0.004
2024-12-25 06:47:08.093 | INFO     | __main__:main:221 - scheduler                      cosine
scheduler                      cosine
2024-12-25 06:47:08.093 | INFO     | __main__:main:221 - min_lr_ratio                   0.1
min_lr_ratio                   0.1
2024-12-25 06:47:08.093 | INFO     | __main__:main:221 - activation_checkpointing       False
activation_checkpointing       False
2024-12-25 06:47:08.094 | INFO     | __main__:main:221 - weight_decay                   0.0
weight_decay                   0.0
2024-12-25 06:47:08.094 | INFO     | __main__:main:221 - warmup_steps                   1000
warmup_steps                   1000
2024-12-25 06:47:08.094 | INFO     | __main__:main:221 - eval_every                     1000
eval_every                     1000
2024-12-25 06:47:08.094 | INFO     | __main__:main:221 - num_training_steps             20000
num_training_steps             20000
2024-12-25 06:47:08.094 | INFO     | __main__:main:221 - max_train_tokens               None
max_train_tokens               None
2024-12-25 06:47:08.094 | INFO     | __main__:main:221 - save_every                     2000
save_every                     2000
2024-12-25 06:47:08.094 | INFO     | __main__:main:221 - save_dir                       /scratch-shared/HTJ2/checkpoints/new0_9144440
save_dir                       /scratch-shared/HTJ2/checkpoints/new0_9144440
2024-12-25 06:47:08.094 | INFO     | __main__:main:221 - tags                           None
tags                           None
2024-12-25 06:47:08.095 | INFO     | __main__:main:221 - dtype                          bfloat16
dtype                          bfloat16
2024-12-25 06:47:08.095 | INFO     | __main__:main:221 - workers                        8
workers                        8
2024-12-25 06:47:08.095 | INFO     | __main__:main:221 - seed                           0
seed                           0
2024-12-25 06:47:08.095 | INFO     | __main__:main:221 - name                           test
name                           test
2024-12-25 06:47:08.095 | INFO     | __main__:main:221 - model_type                     llama
model_type                     llama
2024-12-25 06:47:08.095 | INFO     | __main__:main:221 - grad_clipping                  0.0
grad_clipping                  0.0
2024-12-25 06:47:08.095 | INFO     | __main__:main:221 - beta1                          0.0
beta1                          0.0
2024-12-25 06:47:08.096 | INFO     | __main__:main:221 - rank                           0.3225388601036269
rank                           0.3225388601036269
2024-12-25 06:47:08.096 | INFO     | __main__:main:221 - update_proj_gap                500
update_proj_gap                500
2024-12-25 06:47:08.096 | INFO     | __main__:main:221 - galore_scale                   1.0
galore_scale                   1.0
2024-12-25 06:47:08.096 | INFO     | __main__:main:221 - updating_mask_method           interaction
updating_mask_method           interaction
2024-12-25 06:47:08.096 | INFO     | __main__:main:221 - init_mask                      random
init_mask                      random
2024-12-25 06:47:08.096 | INFO     | __main__:main:221 - single_gpu                     False
single_gpu                     False
2024-12-25 06:47:08.096 | INFO     | __main__:main:221 - mask_grad                      False
mask_grad                      False
2024-12-25 06:47:08.096 | INFO     | __main__:main:221 - spike_clip                     True
spike_clip                     True
2024-12-25 06:47:08.096 | INFO     | __main__:main:221 - sampling                       False
sampling                       False
2024-12-25 06:47:08.097 | INFO     | __main__:main:221 - load_local                     False
load_local                     False
2024-12-25 06:47:08.097 | INFO     | __main__:main:221 - m_replace                      False
m_replace                      False
2024-12-25 06:47:08.097 | INFO     | __main__:main:221 - warmup_epoch                   150
warmup_epoch                   150
2024-12-25 06:47:08.097 | INFO     | __main__:main:221 - threshold                      5000.0
threshold                      5000.0
2024-12-25 06:47:08.097 | INFO     | __main__:main:221 - grad_accu_steps                20.0
grad_accu_steps                20.0
2024-12-25 06:47:08.097 | INFO     | __main__:main:223 - ****************************************
2024-12-25 06:47:20.041 | INFO     | __main__:main:236 - Shuffling data with seed 42
/home/huangti/miniconda3/envs/spam/lib/python3.11/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
Update steps:   0%|                                   | 0/20000 [00:00<?, ?it/s]2024-12-25 06:47:21.455 | INFO     | __main__:main:356 -
LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32000, 512, padding_idx=31999)
    (layers): ModuleList(
      (0-7): 8 x LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=512, out_features=512, bias=False)
          (k_proj): Linear(in_features=512, out_features=512, bias=False)
          (v_proj): Linear(in_features=512, out_features=512, bias=False)
          (o_proj): Linear(in_features=512, out_features=512, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=512, out_features=1376, bias=False)
          (down_proj): Linear(in_features=1376, out_features=512, bias=False)
          (up_proj): Linear(in_features=512, out_features=1376, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=512, out_features=32000, bias=False)
)

2024-12-25 06:47:21.455 | INFO     | __main__:main:357 - Total params: 58.07M
2024-12-25 06:47:21.456 | INFO     | __main__:main:358 - Trainable params: 58.07M
2024-12-25 06:47:21.456 | INFO     | __main__:main:361 - Saving model to /scratch-shared/HTJ2/checkpoints/new0_9144440 every 2000 update steps
/gpfs/home2/huangti/SPAM_v2/galore_torch/SPAM.py:68: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
density 0.3225375951262953
/home/huangti/miniconda3/envs/spam/lib/python3.11/site-packages/torch/autograd/graph.py:825: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at /opt/conda/conda-bld/pytorch_1728945379270/work/aten/src/ATen/native/cudnn/MHA.cpp:674.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass

loss at 4 2.625
Update steps:   2%|▌                      | 500/20000 [02:59<2:12:34,  2.45it/s]/home/huangti/miniconda3/envs/spam/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:224: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate

loss at 404 1.796875

loss at 804 1.5546875

loss at 1204 1.375

loss at 1604 1.265625
lr 0.001992
lr 0.001992
Mask overlap ratio: 1.00
Mask Update
  warnings.warn(
/home/huangti/miniconda3/envs/spam/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

loss at 2004 1.203125
Update steps:   5%|█                     | 1000/20000 [05:49<2:04:24,  2.55it/s]2024-12-25 06:53:11.184 | INFO     | __main__:main:564 - Performing evaluation at step 1000

loss at 2404 1.3515625

loss at 2804 1.4765625

loss at 3204 1.375

loss at 3604 1.3125
lr 0.003992
lr 0.003992
Mask overlap ratio: 3.10
Mask Update
2024-12-25 06:53:18.387 | INFO     | __main__:evaluate_model:113 - Loaded validation dataset in 7.20 seconds
2024-12-25 06:53:18.389 | INFO     | __main__:evaluate_model:129 - Eval set prepared in 7.20 seconds
2024-12-25 06:53:53.897 | INFO     | __main__:main:576 - Eval loss at step 1000: 5.021197319030762,tokens_seen: 99979486

loss at 4004 1.25
Update steps:  10%|██▏                   | 2000/20000 [12:13<1:57:43,  2.55it/s]2024-12-25 06:59:35.836 | INFO     | __main__:main:564 - Performing evaluation at step 2000

loss at 4404 1.234375

loss at 4804 1.2109375

loss at 5204 1.25

loss at 5604 1.2421875
lr 0.0039939011446085354
lr 0.0039939011446085354
Mask overlap ratio: 3.10
Mask Update

loss at 6004 1.234375

loss at 6404 1.265625

loss at 6804 1.4609375

loss at 7204 1.390625

loss at 7604 1.4765625
lr 0.00397554822383021
lr 0.00397554822383021
Mask overlap ratio: 3.10
Mask Update
2024-12-25 06:59:42.625 | INFO     | __main__:evaluate_model:113 - Loaded validation dataset in 6.79 seconds
2024-12-25 06:59:42.626 | INFO     | __main__:evaluate_model:129 - Eval set prepared in 6.79 seconds
2024-12-25 07:00:14.915 | INFO     | __main__:main:576 - Eval loss at step 2000: 6.181187629699707,tokens_seen: 199880081

loss at 8004 1.5546875
Update steps:  15%|███▎                  | 3000/20000 [18:36<1:51:52,  2.53it/s]2024-12-25 07:05:57.516 | INFO     | __main__:main:564 - Performing evaluation at step 3000

loss at 8404 1.546875

loss at 8804 1.5859375

loss at 9204 1.484375

loss at 9604 1.4921875
lr 0.003945066508300911
lr 0.003945066508300911
Mask overlap ratio: 3.10
Mask Update

loss at 10004 1.4609375

loss at 10404 1.4921875

loss at 10804 1.5234375

loss at 11204 1.453125

loss at 11604 1.46875
lr 0.0039026642190457565
lr 0.0039026642190457565
Mask overlap ratio: 3.10
Mask Update
2024-12-25 07:06:01.671 | INFO     | __main__:evaluate_model:113 - Loaded validation dataset in 4.15 seconds
2024-12-25 07:06:01.672 | INFO     | __main__:evaluate_model:129 - Eval set prepared in 4.16 seconds
2024-12-25 07:06:33.982 | INFO     | __main__:main:576 - Eval loss at step 3000: 5.753896713256836,tokens_seen: 299903185

loss at 12004 1.421875
Update steps:  20%|████▍                 | 4000/20000 [24:55<1:45:09,  2.54it/s]2024-12-25 07:12:17.048 | INFO     | __main__:main:564 - Performing evaluation at step 4000

loss at 12404 1.4609375

loss at 12804 1.4921875

loss at 13204 1.484375

loss at 13604 1.5078125
lr 0.0038486310066957134
lr 0.0038486310066957134
Mask overlap ratio: 3.10
Mask Update

loss at 14004 1.4375

loss at 14404 1.4453125

loss at 14804 1.5390625

loss at 15204 1.7265625

loss at 15604 1.6953125
lr 0.003783335972880091
lr 0.003783335972880091
Mask overlap ratio: 3.10
Mask Update
2024-12-25 07:12:20.942 | INFO     | __main__:evaluate_model:113 - Loaded validation dataset in 3.89 seconds
2024-12-25 07:12:20.944 | INFO     | __main__:evaluate_model:129 - Eval set prepared in 3.90 seconds
2024-12-25 07:12:53.342 | INFO     | __main__:main:576 - Eval loss at step 4000: 7.068812370300293,tokens_seen: 399934286

loss at 16004 1.78125
Update steps:  25%|█████▌                | 5000/20000 [31:14<1:38:50,  2.53it/s]2024-12-25 07:18:36.482 | INFO     | __main__:main:564 - Performing evaluation at step 5000

loss at 16404 1.7734375

loss at 16804 1.7578125

loss at 17204 1.75

loss at 17604 1.7109375
lr 0.003707225148888141
lr 0.003707225148888141
Mask overlap ratio: 3.10
Mask Update

loss at 18004 1.71875

loss at 18404 1.7265625

loss at 18804 1.7578125

loss at 19204 1.7421875

loss at 19604 1.734375
lr 0.003620818448823091
lr 0.003620818448823091
Mask overlap ratio: 3.10
Mask Update
2024-12-25 07:18:40.483 | INFO     | __main__:evaluate_model:113 - Loaded validation dataset in 4.00 seconds
2024-12-25 07:18:40.484 | INFO     | __main__:evaluate_model:129 - Eval set prepared in 4.00 seconds
2024-12-25 07:19:12.402 | INFO     | __main__:main:576 - Eval loss at step 5000: 6.917238235473633,tokens_seen: 499986358

loss at 20004 1.71875
Update steps:  30%|██████▌               | 6000/20000 [37:34<1:32:06,  2.53it/s]2024-12-25 07:24:56.419 | INFO     | __main__:main:564 - Performing evaluation at step 6000

loss at 20404 1.7265625

loss at 20804 1.7578125

loss at 21204 1.7265625

loss at 21604 1.734375
lr 0.0035247061180616254
lr 0.0035247061180616254
Mask overlap ratio: 3.10
Mask Update

loss at 22004 1.734375

loss at 22404 1.703125

loss at 22804 1.75

loss at 23204 1.734375

loss at 23604 1.75
lr 0.0034195447012794664
lr 0.0034195447012794664
Mask overlap ratio: 3.10
Mask Update
2024-12-25 07:25:00.540 | INFO     | __main__:evaluate_model:113 - Loaded validation dataset in 4.12 seconds
2024-12-25 07:25:00.542 | INFO     | __main__:evaluate_model:129 - Eval set prepared in 4.12 seconds
2024-12-25 07:25:32.831 | INFO     | __main__:main:576 - Eval loss at step 6000: 6.883728504180908,tokens_seen: 600040686

loss at 24004 1.71875
Update steps:  35%|███████▋              | 7000/20000 [43:54<1:25:30,  2.53it/s]2024-12-25 07:31:15.757 | INFO     | __main__:main:564 - Performing evaluation at step 7000

loss at 24404 1.7265625

loss at 24804 1.7265625

loss at 25204 1.75

loss at 25604 1.7421875
lr 0.00330605255758551
lr 0.00330605255758551
Mask overlap ratio: 3.10
Mask Update

loss at 26004 1.75

loss at 26404 1.734375

loss at 26804 1.7421875

loss at 27204 1.7734375

loss at 27604 1.765625
lr 0.003185004953400705
lr 0.003185004953400705
Mask overlap ratio: 3.10
Mask Update
2024-12-25 07:31:19.750 | INFO     | __main__:evaluate_model:113 - Loaded validation dataset in 3.99 seconds
2024-12-25 07:31:19.751 | INFO     | __main__:evaluate_model:129 - Eval set prepared in 3.99 seconds
2024-12-25 07:31:51.911 | INFO     | __main__:main:576 - Eval loss at step 7000: 7.050888538360596,tokens_seen: 700029429

loss at 28004 1.765625
Update steps:  40%|████████▊             | 8000/20000 [50:13<1:19:46,  2.51it/s]2024-12-25 07:37:35.032 | INFO     | __main__:main:564 - Performing evaluation at step 8000

loss at 28404 1.765625

loss at 28804 1.734375

loss at 29204 1.7421875

loss at 29604 1.75
lr 0.0030572287666022903
lr 0.0030572287666022903
Mask overlap ratio: 3.10
Mask Update

loss at 30004 1.765625

loss at 30404 1.75

loss at 30804 1.75

loss at 31204 1.75

loss at 31604 1.734375
lr 0.002923596838109448
lr 0.002923596838109448
Mask overlap ratio: 3.10
Mask Update
2024-12-25 07:37:39.335 | INFO     | __main__:evaluate_model:113 - Loaded validation dataset in 4.30 seconds
2024-12-25 07:37:39.336 | INFO     | __main__:evaluate_model:129 - Eval set prepared in 4.30 seconds
2024-12-25 07:38:11.648 | INFO     | __main__:main:576 - Eval loss at step 8000: 6.963684558868408,tokens_seen: 800038852

loss at 32004 1.765625
Update steps:  45%|█████████▉            | 9000/20000 [56:33<1:12:34,  2.53it/s]2024-12-25 07:43:54.658 | INFO     | __main__:main:564 - Performing evaluation at step 9000

loss at 32404 1.734375

loss at 32804 1.734375

loss at 33204 1.734375

loss at 33604 1.75
lr 0.002785022009494777
lr 0.002785022009494777
Mask overlap ratio: 3.10
Mask Update

loss at 34004 1.734375

loss at 34404 1.7421875

loss at 34804 1.734375

loss at 35204 1.7578125

loss at 35604 1.7265625
lr 0.0026424508873507425
lr 0.0026424508873507425
Mask overlap ratio: 3.10
Mask Update
2024-12-25 07:43:57.979 | INFO     | __main__:evaluate_model:113 - Loaded validation dataset in 3.32 seconds
2024-12-25 07:43:57.980 | INFO     | __main__:evaluate_model:129 - Eval set prepared in 3.32 seconds
2024-12-25 07:44:29.563 | INFO     | __main__:main:576 - Eval loss at step 9000: 6.915835857391357,tokens_seen: 899981426

loss at 36004 1.71875
Update steps:  50%|█████████▌         | 10000/20000 [1:02:51<1:05:53,  2.53it/s]2024-12-25 07:50:13.184 | INFO     | __main__:main:564 - Performing evaluation at step 10000

loss at 36404 1.734375

loss at 36804 1.7734375

loss at 37204 1.765625

loss at 37604 1.765625
lr 0.0024968573770068054
lr 0.0024968573770068054
Mask overlap ratio: 3.10
Mask Update

loss at 38004 1.7578125

loss at 38404 1.7734375

loss at 38804 1.7578125

loss at 39204 1.7421875

loss at 39604 1.734375
lr 0.0023492360297684917
lr 0.0023492360297684917
Mask overlap ratio: 3.10
Mask Update
2024-12-25 07:50:17.477 | INFO     | __main__:evaluate_model:113 - Loaded validation dataset in 4.29 seconds
2024-12-25 07:50:17.479 | INFO     | __main__:evaluate_model:129 - Eval set prepared in 4.29 seconds
2024-12-25 07:50:50.086 | INFO     | __main__:main:576 - Eval loss at step 10000: 6.970464706420898,tokens_seen: 999938455

loss at 40004 1.734375
Update steps:  55%|███████████▌         | 11000/20000 [1:09:11<59:10,  2.53it/s]2024-12-25 07:56:33.086 | INFO     | __main__:main:564 - Performing evaluation at step 11000

loss at 40404 1.7734375

loss at 40804 1.7421875

loss at 41204 1.71875

loss at 41604 1.734375
lr 0.002200595249123515
lr 0.002200595249123515
Mask overlap ratio: 3.10
Mask Update

loss at 42004 1.75

loss at 42404 1.7265625

loss at 42804 1.734375

loss at 43204 1.75

loss at 43604 1.75
lr 0.002051950402323451
lr 0.002051950402323451
Mask overlap ratio: 3.10
Mask Update
2024-12-25 07:56:36.770 | INFO     | __main__:evaluate_model:113 - Loaded validation dataset in 3.68 seconds
2024-12-25 07:56:36.772 | INFO     | __main__:evaluate_model:129 - Eval set prepared in 3.69 seconds
2024-12-25 07:57:09.402 | INFO     | __main__:main:576 - Eval loss at step 11000: 6.9404616355896,tokens_seen: 1099986729

loss at 44004 1.7421875
Update steps:  60%|████████████▌        | 12000/20000 [1:15:30<52:42,  2.53it/s]2024-12-25 08:02:52.696 | INFO     | __main__:main:564 - Performing evaluation at step 12000

loss at 44404 1.765625

loss at 44804 1.7421875

loss at 45204 1.71875

loss at 45604 1.75
lr 0.0019043168843958344
lr 0.0019043168843958344
Mask overlap ratio: 3.10
Mask Update

loss at 46004 1.7265625

loss at 46404 1.7578125

loss at 46804 1.7265625

loss at 47204 1.734375

loss at 47604 1.71875
lr 0.0017587031819665288
lr 0.0017587031819665288
Mask overlap ratio: 3.10
Mask Update
2024-12-25 08:02:58.330 | INFO     | __main__:evaluate_model:113 - Loaded validation dataset in 5.63 seconds
2024-12-25 08:02:58.331 | INFO     | __main__:evaluate_model:129 - Eval set prepared in 5.64 seconds
2024-12-25 08:03:29.969 | INFO     | __main__:main:576 - Eval loss at step 12000: 6.902041912078857,tokens_seen: 1199964331

loss at 48004 1.71875
Update steps:  65%|█████████████▋       | 13000/20000 [1:21:51<46:06,  2.53it/s]2024-12-25 08:09:12.622 | INFO     | __main__:main:564 - Performing evaluation at step 13000

loss at 48404 1.734375

loss at 48804 1.75

loss at 49204 1.7109375

loss at 49604 1.7421875
lr 0.0016161039842734785
lr 0.0016161039842734785
Mask overlap ratio: 3.10
Mask Update

loss at 50004 1.734375

loss at 50404 1.71875

loss at 50804 1.734375

loss at 51204 1.734375

loss at 51604 1.71875
lr 0.0014774933884306122
lr 0.0014774933884306122
Mask overlap ratio: 3.10
Mask Update
2024-12-25 08:09:16.479 | INFO     | __main__:evaluate_model:113 - Loaded validation dataset in 3.86 seconds
2024-12-25 08:09:16.481 | INFO     | __main__:evaluate_model:129 - Eval set prepared in 3.86 seconds
2024-12-25 08:09:48.688 | INFO     | __main__:main:576 - Eval loss at step 13000: 6.882793426513672,tokens_seen: 1300025548

loss at 52004 1.7421875
Update steps:  70%|██████████████▋      | 14000/20000 [1:28:10<39:33,  2.53it/s]2024-12-25 08:15:32.375 | INFO     | __main__:main:564 - Performing evaluation at step 14000

loss at 52404 1.7265625

loss at 52804 1.71875

loss at 53204 1.71875

loss at 53604 1.7421875
lr 0.0013438182453568302
lr 0.0013438182453568302
Mask overlap ratio: 3.10
Mask Update

loss at 54004 1.71875

loss at 54404 1.7265625

loss at 54804 1.734375

loss at 55204 1.7265625

loss at 55604 1.7578125
lr 0.0012159916918241392
lr 0.0012159916918241392
Mask overlap ratio: 3.10
Mask Update
2024-12-25 08:15:36.091 | INFO     | __main__:evaluate_model:113 - Loaded validation dataset in 3.72 seconds
2024-12-25 08:15:36.092 | INFO     | __main__:evaluate_model:129 - Eval set prepared in 3.72 seconds
2024-12-25 08:16:08.687 | INFO     | __main__:main:576 - Eval loss at step 14000: 6.900873184204102,tokens_seen: 1400046797

loss at 56004 1.734375
Update steps:  75%|███████████████▊     | 15000/20000 [1:34:30<32:58,  2.53it/s]2024-12-25 08:21:51.946 | INFO     | __main__:main:564 - Performing evaluation at step 15000

loss at 56404 1.71875

loss at 56804 1.7265625

loss at 57204 1.7265625

loss at 57604 1.71875
lr 0.0010948869128075875
lr 0.0010948869128075875
Mask overlap ratio: 3.10
Mask Update

loss at 58004 1.7265625

loss at 58404 1.734375

loss at 58804 1.7265625

loss at 59204 1.7109375

loss at 59604 1.71875
lr 0.0009813311767464888
lr 0.0009813311767464888
Mask overlap ratio: 3.10
Mask Update
2024-12-25 08:21:56.435 | INFO     | __main__:evaluate_model:113 - Loaded validation dataset in 4.49 seconds
2024-12-25 08:21:56.437 | INFO     | __main__:evaluate_model:129 - Eval set prepared in 4.49 seconds
2024-12-25 08:22:28.340 | INFO     | __main__:main:576 - Eval loss at step 15000: 6.902353763580322,tokens_seen: 1500063597

loss at 60004 1.734375
Update steps:  80%|████████████████▊    | 16000/20000 [1:40:49<26:19,  2.53it/s]2024-12-25 08:28:11.340 | INFO     | __main__:main:564 - Performing evaluation at step 16000

loss at 60404 1.7265625

loss at 60804 1.7265625

loss at 61204 1.71875

loss at 61604 1.7265625
lr 0.0008761001844621411
lr 0.0008761001844621411
Mask overlap ratio: 3.10
Mask Update

loss at 62004 1.71875

loss at 62404 1.71875

loss at 62804 1.7109375

loss at 63204 1.734375

loss at 63604 1.71875
lr 0.0007799127703346695
lr 0.0007799127703346695
Mask overlap ratio: 3.10
Mask Update
2024-12-25 08:28:16.201 | INFO     | __main__:evaluate_model:113 - Loaded validation dataset in 4.86 seconds
2024-12-25 08:28:16.203 | INFO     | __main__:evaluate_model:129 - Eval set prepared in 4.86 seconds
2024-12-25 08:28:47.893 | INFO     | __main__:main:576 - Eval loss at step 16000: 6.896820545196533,tokens_seen: 1600154706

loss at 64004 1.734375
Update steps:  85%|█████████████████▊   | 17000/20000 [1:47:08<19:42,  2.54it/s]2024-12-25 08:34:30.421 | INFO     | __main__:main:564 - Performing evaluation at step 17000

loss at 64404 1.7265625

loss at 64804 1.7265625

loss at 65204 1.7265625

loss at 65604 1.7265625
lr 0.0006934259919353199
lr 0.0006934259919353199
Mask overlap ratio: 3.10
Mask Update

loss at 66004 1.75

loss at 66404 1.71875

loss at 66804 1.7265625

loss at 67204 1.71875

loss at 67604 1.734375
lr 0.0006172306416569991
lr 0.0006172306416569991
Mask overlap ratio: 3.10
Mask Update
2024-12-25 08:34:34.264 | INFO     | __main__:evaluate_model:113 - Loaded validation dataset in 3.84 seconds
2024-12-25 08:34:34.265 | INFO     | __main__:evaluate_model:129 - Eval set prepared in 3.84 seconds
2024-12-25 08:35:06.599 | INFO     | __main__:main:576 - Eval loss at step 17000: 6.887157440185547,tokens_seen: 1700164802

loss at 68004 1.734375
Update steps:  90%|██████████████████▉  | 18000/20000 [1:53:27<13:09,  2.53it/s]2024-12-25 08:40:49.426 | INFO     | __main__:main:564 - Performing evaluation at step 18000

loss at 68404 1.7265625

loss at 68804 1.734375

loss at 69204 1.734375

loss at 69604 1.71875
lr 0.0005518472110031761
lr 0.0005518472110031761
Mask overlap ratio: 3.10
Mask Update

loss at 70004 1.703125

loss at 70404 1.7109375

loss at 70804 1.7421875

loss at 71204 1.7265625

loss at 71604 1.7265625
lr 0.0004977223351031438
lr 0.0004977223351031438
Mask overlap ratio: 3.10
Mask Update
2024-12-25 08:40:52.703 | INFO     | __main__:evaluate_model:113 - Loaded validation dataset in 3.28 seconds
2024-12-25 08:40:52.705 | INFO     | __main__:evaluate_model:129 - Eval set prepared in 3.28 seconds
2024-12-25 08:41:25.035 | INFO     | __main__:main:576 - Eval loss at step 18000: 6.888248443603516,tokens_seen: 1800161308

loss at 72004 1.7265625
Update steps:  95%|███████████████████▉ | 19000/20000 [1:59:46<06:34,  2.53it/s]2024-12-25 08:47:07.613 | INFO     | __main__:main:564 - Performing evaluation at step 19000

loss at 72404 1.7421875

loss at 72804 1.71875

loss at 73204 1.7265625

loss at 73604 1.7265625
lr 0.0004552257417412016
lr 0.0004552257417412016
Mask overlap ratio: 3.10
Mask Update

loss at 74004 1.7265625

loss at 74404 1.703125

loss at 74804 1.7265625

loss at 75204 1.71875

loss at 75604 1.7421875
lr 0.00042464772574098993
lr 0.00042464772574098993
Mask overlap ratio: 3.10
Mask Update
2024-12-25 08:47:11.600 | INFO     | __main__:evaluate_model:113 - Loaded validation dataset in 3.99 seconds
2024-12-25 08:47:11.602 | INFO     | __main__:evaluate_model:129 - Eval set prepared in 3.99 seconds
2024-12-25 08:47:43.721 | INFO     | __main__:main:576 - Eval loss at step 19000: 6.885209083557129,tokens_seen: 1900163851

loss at 76004 1.7265625
Update steps: 100%|█████████████████████| 20000/20000 [2:06:04<00:00,  2.54it/s]2024-12-25 08:53:26.688 | INFO     | __main__:main:564 - Performing evaluation at step 20000

loss at 76404 1.7265625

loss at 76804 1.7265625

loss at 77204 1.734375

loss at 77604 1.734375
lr 0.00040619716595749077
lr 0.00040619716595749077
Mask overlap ratio: 3.10
Mask Update

loss at 78004 1.734375

loss at 78404 1.7421875

loss at 78804 1.7265625

loss at 79204 1.71875

loss at 79604 1.7109375
lr 0.00040000009842264684
lr 0.00040000009842264684
Mask overlap ratio: 3.10
Mask Update
2024-12-25 08:53:31.237 | INFO     | __main__:evaluate_model:113 - Loaded validation dataset in 4.55 seconds
2024-12-25 08:53:31.239 | INFO     | __main__:evaluate_model:129 - Eval set prepared in 4.55 seconds
2024-12-25 08:54:03.284 | INFO     | __main__:main:576 - Eval loss at step 20000: 6.885131359100342,tokens_seen: 2000123948

loss at 80004 1.71875
Update steps: 20001it [2:06:42, 11.50s/it]                                      2024-12-25 08:54:03.620 | INFO     | __main__:main:493 - Reached max number of update steps (f20000). Stopping training.
Rank 0 stopping training.
2024-12-25 08:54:04.002 | INFO     | __main__:main:603 - Training finished
Update steps: 20001it [2:06:42,  2.63it/s]
2024-12-25 08:54:04.003 | INFO     | __main__:main:608 - Saving model and optimizer to /scratch-shared/HTJ2/checkpoints/new0_9144440/model_20001, update step 20001
2024-12-25 08:54:04.374 | INFO     | __main__:main:635 - Running final evaluation
2024-12-25 08:54:10.086 | INFO     | __main__:evaluate_model:113 - Loaded validation dataset in 5.53 seconds
2024-12-25 08:54:10.087 | INFO     | __main__:evaluate_model:129 - Eval set prepared in 5.54 seconds
2024-12-25 08:54:42.347 | INFO     | __main__:main:653 - Final eval loss: 6.885131359100342
2024-12-25 08:54:42.348 | INFO     | __main__:main:655 - Script finished successfully
Rank 0 finished successfully
perplexity 977.6300818825291
