[32m2025-01-14 10:08:44.423[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m209[0m - [1mUsing dist with rank 0 (only rank 0 will log)[0m
[32m2025-01-14 10:08:44.423[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m210[0m - [1m****************************************[0m
[32m2025-01-14 10:08:44.423[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m211[0m - [1mStarting training with the arguments[0m
[32m2025-01-14 10:08:44.423[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m213[0m - [1mmodel_config                   configs/llama_60m.json[0m
model_config                   configs/llama_60m.json
[32m2025-01-14 10:08:44.424[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m213[0m - [1muse_hf_model                   False[0m
use_hf_model                   False
[32m2025-01-14 10:08:44.424[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m213[0m - [1mcontinue_from                  None[0m
continue_from                  None
[32m2025-01-14 10:08:44.424[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m213[0m - [1mbatch_size                     128[0m
batch_size                     128
[32m2025-01-14 10:08:44.424[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m213[0m - [1mgradient_accumulation          4[0m
gradient_accumulation          4
[32m2025-01-14 10:08:44.424[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m213[0m - [1mtotal_batch_size               512[0m
total_batch_size               512
[32m2025-01-14 10:08:44.424[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m213[0m - [1mmax_length                     256[0m
max_length                     256
[32m2025-01-14 10:08:44.424[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m213[0m - [1moptimizer                      SPAM[0m
optimizer                      SPAM
[32m2025-01-14 10:08:44.424[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m213[0m - [1mlr                             0.004[0m
lr                             0.004
[32m2025-01-14 10:08:44.424[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m213[0m - [1mscheduler                      cosine[0m
scheduler                      cosine
[32m2025-01-14 10:08:44.424[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m213[0m - [1mmin_lr_ratio                   0.1[0m
min_lr_ratio                   0.1
[32m2025-01-14 10:08:44.425[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m213[0m - [1mactivation_checkpointing       False[0m
activation_checkpointing       False
[32m2025-01-14 10:08:44.425[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m213[0m - [1mweight_decay                   0.0[0m
weight_decay                   0.0
[32m2025-01-14 10:08:44.425[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m213[0m - [1mwarmup_steps                   1000[0m
warmup_steps                   1000
[32m2025-01-14 10:08:44.425[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m213[0m - [1meval_every                     1000[0m
eval_every                     1000
[32m2025-01-14 10:08:44.425[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m213[0m - [1mnum_training_steps             20000[0m
num_training_steps             20000
[32m2025-01-14 10:08:44.425[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m213[0m - [1mmax_train_tokens               None[0m
max_train_tokens               None
[32m2025-01-14 10:08:44.425[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m213[0m - [1msave_every                     2000[0m
save_every                     2000
[32m2025-01-14 10:08:44.425[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m213[0m - [1msave_dir                       /scratch-shared/HTJ2/checkpoints/new0_9390469[0m
save_dir                       /scratch-shared/HTJ2/checkpoints/new0_9390469
[32m2025-01-14 10:08:44.425[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m213[0m - [1mtags                           None[0m
tags                           None
[32m2025-01-14 10:08:44.425[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m213[0m - [1mdtype                          bfloat16[0m
dtype                          bfloat16
[32m2025-01-14 10:08:44.425[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m213[0m - [1mworkers                        8[0m
workers                        8
[32m2025-01-14 10:08:44.425[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m213[0m - [1mseed                           0[0m
seed                           0
[32m2025-01-14 10:08:44.425[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m213[0m - [1mname                           test[0m
name                           test
[32m2025-01-14 10:08:44.426[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m213[0m - [1mmodel_type                     llama[0m
model_type                     llama
[32m2025-01-14 10:08:44.426[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m213[0m - [1mgrad_clipping                  0.0[0m
grad_clipping                  0.0
[32m2025-01-14 10:08:44.426[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m213[0m - [1mbeta1                          0.0[0m
beta1                          0.0
[32m2025-01-14 10:08:44.426[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m213[0m - [1mgalore_scale                   1.0[0m
galore_scale                   1.0
[32m2025-01-14 10:08:44.426[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m213[0m - [1msingle_gpu                     False[0m
single_gpu                     False
[32m2025-01-14 10:08:44.426[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m213[0m - [1mload_local                     False[0m
load_local                     False
[32m2025-01-14 10:08:44.426[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m213[0m - [1mwarmup_epoch                   150[0m
warmup_epoch                   150
[32m2025-01-14 10:08:44.426[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m213[0m - [1mthreshold                      5000.0[0m
threshold                      5000.0
[32m2025-01-14 10:08:44.426[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m213[0m - [1mgrad_accu_steps                20[0m
grad_accu_steps                20
[32m2025-01-14 10:08:44.426[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m213[0m - [1mdensity                        1.0[0m
density                        1.0
[32m2025-01-14 10:08:44.426[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m213[0m - [1mupdate_gap                     500[0m
update_gap                     500
[32m2025-01-14 10:08:44.426[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m215[0m - [1m****************************************[0m
Resolving data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1024/1024 [00:02<00:00, 378.56it/s]
Resolving data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1024/1024 [00:00<00:00, 523521.12it/s]
[32m2025-01-14 10:08:53.259[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m228[0m - [1mShuffling data with seed 42[0m
/home/huangti/miniconda3/envs/spam/lib/python3.11/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[34m[1mwandb[0m: [33mWARNING[0m Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
Update steps:   0%|                                   | 0/20000 [00:00<?, ?it/s][32m2025-01-14 10:08:54.697[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m345[0m - [1m
LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32000, 512, padding_idx=31999)
    (layers): ModuleList(
      (0-7): 8 x LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=512, out_features=512, bias=False)
          (k_proj): Linear(in_features=512, out_features=512, bias=False)
          (v_proj): Linear(in_features=512, out_features=512, bias=False)
          (o_proj): Linear(in_features=512, out_features=512, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=512, out_features=1376, bias=False)
          (down_proj): Linear(in_features=1376, out_features=512, bias=False)
          (up_proj): Linear(in_features=512, out_features=1376, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=512, out_features=32000, bias=False)
)
[0m
[32m2025-01-14 10:08:54.698[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m346[0m - [1mTotal params: 58.07M[0m
[32m2025-01-14 10:08:54.698[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m347[0m - [1mTrainable params: 58.07M[0m
[32m2025-01-14 10:08:54.698[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m350[0m - [1mSaving model to /scratch-shared/HTJ2/checkpoints/new0_9390469 every 2000 update steps[0m
/gpfs/home2/huangti/SPAM_v2/galore_torch/SPAM.py:104: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use `torch.optim.AdamW` instead, or set `no_deprecation_warning=True` to disable this warning.
  warnings.warn(
density 1.0
/home/huangti/miniconda3/envs/spam/lib/python3.11/site-packages/torch/autograd/graph.py:825: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at /opt/conda/conda-bld/pytorch_1728945379270/work/aten/src/ATen/native/cudnn/MHA.cpp:674.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass

loss at 4 2.625
Update steps:   0%|                        | 1/20000 [00:06<34:07:17,  6.14s/it]Traceback (most recent call last):
  File "/gpfs/home2/huangti/SPAM_v2/torchrun_main.py", line 652, in <module>
    main(args)
  File "/gpfs/home2/huangti/SPAM_v2/torchrun_main.py", line 510, in main
    if args.mask_grad:
       ^^^^^^^^^^^^^^
AttributeError: 'Namespace' object has no attribute 'mask_grad'
[rank0]: Traceback (most recent call last):
[rank0]:   File "/gpfs/home2/huangti/SPAM_v2/torchrun_main.py", line 652, in <module>
[rank0]:     main(args)
[rank0]:   File "/gpfs/home2/huangti/SPAM_v2/torchrun_main.py", line 510, in main
[rank0]:     if args.mask_grad:
[rank0]:        ^^^^^^^^^^^^^^
[rank0]: AttributeError: 'Namespace' object has no attribute 'mask_grad'
