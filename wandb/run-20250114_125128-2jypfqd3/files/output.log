2025-01-14 12:51:29.417 | INFO     | __main__:main:209 - Using dist with rank 0 (only rank 0 will log)
2025-01-14 12:51:29.418 | INFO     | __main__:main:210 - ****************************************
2025-01-14 12:51:29.418 | INFO     | __main__:main:211 - Starting training with the arguments
2025-01-14 12:51:29.418 | INFO     | __main__:main:213 - model_config                   configs/llama_130m.json
model_config                   configs/llama_130m.json
2025-01-14 12:51:29.418 | INFO     | __main__:main:213 - use_hf_model                   False
use_hf_model                   False
2025-01-14 12:51:29.418 | INFO     | __main__:main:213 - continue_from                  None
continue_from                  None
2025-01-14 12:51:29.418 | INFO     | __main__:main:213 - batch_size                     128
batch_size                     128
2025-01-14 12:51:29.419 | INFO     | __main__:main:213 - gradient_accumulation          2
gradient_accumulation          2
2025-01-14 12:51:29.419 | INFO     | __main__:main:213 - total_batch_size               512
total_batch_size               512
2025-01-14 12:51:29.419 | INFO     | __main__:main:213 - max_length                     256
max_length                     256
2025-01-14 12:51:29.420 | INFO     | __main__:main:213 - optimizer                      SPAM
optimizer                      SPAM
2025-01-14 12:51:29.421 | INFO     | __main__:main:213 - lr                             0.0008
lr                             0.0008
2025-01-14 12:51:29.421 | INFO     | __main__:main:213 - scheduler                      cosine
scheduler                      cosine
2025-01-14 12:51:29.421 | INFO     | __main__:main:213 - min_lr_ratio                   0.1
min_lr_ratio                   0.1
2025-01-14 12:51:29.421 | INFO     | __main__:main:213 - activation_checkpointing       False
activation_checkpointing       False
2025-01-14 12:51:29.421 | INFO     | __main__:main:213 - weight_decay                   0.0
weight_decay                   0.0
2025-01-14 12:51:29.421 | INFO     | __main__:main:213 - warmup_steps                   1000
warmup_steps                   1000
2025-01-14 12:51:29.421 | INFO     | __main__:main:213 - eval_every                     1000
eval_every                     1000
2025-01-14 12:51:29.421 | INFO     | __main__:main:213 - num_training_steps             20000
num_training_steps             20000
2025-01-14 12:51:29.421 | INFO     | __main__:main:213 - max_train_tokens               None
max_train_tokens               None
2025-01-14 12:51:29.421 | INFO     | __main__:main:213 - save_every                     2000
save_every                     2000
2025-01-14 12:51:29.422 | INFO     | __main__:main:213 - save_dir                       /scratch-shared/HTJ2/checkpoints/new0_9393740
save_dir                       /scratch-shared/HTJ2/checkpoints/new0_9393740
2025-01-14 12:51:29.422 | INFO     | __main__:main:213 - tags                           None
tags                           None
2025-01-14 12:51:29.422 | INFO     | __main__:main:213 - dtype                          bfloat16
dtype                          bfloat16
2025-01-14 12:51:29.422 | INFO     | __main__:main:213 - workers                        8
workers                        8
2025-01-14 12:51:29.422 | INFO     | __main__:main:213 - seed                           0
seed                           0
2025-01-14 12:51:29.422 | INFO     | __main__:main:213 - name                           test
name                           test
2025-01-14 12:51:29.422 | INFO     | __main__:main:213 - model_type                     llama
model_type                     llama
2025-01-14 12:51:29.422 | INFO     | __main__:main:213 - grad_clipping                  0.0
grad_clipping                  0.0
2025-01-14 12:51:29.422 | INFO     | __main__:main:213 - beta1                          0.0
beta1                          0.0
2025-01-14 12:51:29.422 | INFO     | __main__:main:213 - galore_scale                   1.0
galore_scale                   1.0
2025-01-14 12:51:29.422 | INFO     | __main__:main:213 - single_gpu                     False
single_gpu                     False
2025-01-14 12:51:29.422 | INFO     | __main__:main:213 - load_local                     False
load_local                     False
2025-01-14 12:51:29.423 | INFO     | __main__:main:213 - warmup_epoch                   150
warmup_epoch                   150
2025-01-14 12:51:29.423 | INFO     | __main__:main:213 - threshold                      5000.0
threshold                      5000.0
2025-01-14 12:51:29.423 | INFO     | __main__:main:213 - grad_accu_steps                20
grad_accu_steps                20
2025-01-14 12:51:29.423 | INFO     | __main__:main:213 - density                        1.0
density                        1.0
2025-01-14 12:51:29.423 | INFO     | __main__:main:213 - update_gap                     500
update_gap                     500
2025-01-14 12:51:29.423 | INFO     | __main__:main:215 - ****************************************
2025-01-14 12:51:38.594 | INFO     | __main__:main:228 - Shuffling data with seed 42
/home/huangti/miniconda3/envs/spam/lib/python3.11/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
Update steps:   0%|                                   | 0/20000 [00:00<?, ?it/s]2025-01-14 12:51:41.102 | INFO     | __main__:main:345 -
LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32000, 768, padding_idx=31999)
    (layers): ModuleList(
      (0-11): 12 x LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=768, out_features=768, bias=False)
          (k_proj): Linear(in_features=768, out_features=768, bias=False)
          (v_proj): Linear(in_features=768, out_features=768, bias=False)
          (o_proj): Linear(in_features=768, out_features=768, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=768, out_features=2048, bias=False)
          (down_proj): Linear(in_features=2048, out_features=768, bias=False)
          (up_proj): Linear(in_features=768, out_features=2048, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=768, out_features=32000, bias=False)
)

2025-01-14 12:51:41.103 | INFO     | __main__:main:346 - Total params: 134.11M
2025-01-14 12:51:41.103 | INFO     | __main__:main:347 - Trainable params: 134.11M
2025-01-14 12:51:41.103 | INFO     | __main__:main:350 - Saving model to /scratch-shared/HTJ2/checkpoints/new0_9393740 every 2000 update steps
/gpfs/home2/huangti/SPAM_v2/galore_torch/SPAM.py:104: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use `torch.optim.AdamW` instead, or set `no_deprecation_warning=True` to disable this warning.
  warnings.warn(
density 1.0
/home/huangti/miniconda3/envs/spam/lib/python3.11/site-packages/torch/autograd/graph.py:825: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at /opt/conda/conda-bld/pytorch_1728945379270/work/aten/src/ATen/native/cudnn/MHA.cpp:674.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass

loss at 2 5.25
Update steps:   2%|▌                      | 500/20000 [02:53<1:53:23,  2.87it/s]/home/huangti/miniconda3/envs/spam/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:224: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate

loss at 202 3.734375

loss at 402 3.421875

loss at 602 3.125

loss at 802 2.859375
lr 0.00039840000000000003
lr 0.00039840000000000003
Mask overlap ratio: 1.00
Mask Update
  warnings.warn(
/home/huangti/miniconda3/envs/spam/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

loss at 1002 2.671875
Update steps:   5%|█                     | 1000/20000 [05:37<1:48:22,  2.92it/s]2025-01-14 12:57:19.136 | INFO     | __main__:main:496 - Performing evaluation at step 1000

loss at 1202 2.59375

loss at 1402 2.421875

loss at 1602 2.21875

loss at 1802 2.171875
lr 0.0007984
lr 0.0007984
Mask overlap ratio: 1.00
Mask Update
2025-01-14 12:57:24.392 | INFO     | __main__:evaluate_model:105 - Loaded validation dataset in 5.26 seconds
2025-01-14 12:57:24.394 | INFO     | __main__:evaluate_model:121 - Eval set prepared in 5.26 seconds
2025-01-14 12:57:48.009 | INFO     | __main__:main:508 - Eval loss at step 1000: 4.184157371520996,tokens_seen: 99962688

loss at 2002 2.09375
Update steps:  10%|██▏                   | 2000/20000 [11:36<1:42:35,  2.92it/s]2025-01-14 13:03:18.214 | INFO     | __main__:main:496 - Performing evaluation at step 2000

loss at 2202 2.125

loss at 2402 2.0625

loss at 2602 2.03125

loss at 2802 2.015625
lr 0.000798780228921707
lr 0.000798780228921707
Mask overlap ratio: 1.00
Mask Update

loss at 3002 1.9375

loss at 3202 1.9765625

loss at 3402 1.984375

loss at 3602 1.9296875

loss at 3802 1.9140625
lr 0.0007951096447660421
lr 0.0007951096447660421
Mask overlap ratio: 1.00
Mask Update
2025-01-14 13:03:24.184 | INFO     | __main__:evaluate_model:105 - Loaded validation dataset in 5.97 seconds
2025-01-14 13:03:24.186 | INFO     | __main__:evaluate_model:121 - Eval set prepared in 5.97 seconds
2025-01-14 13:03:45.066 | INFO     | __main__:main:508 - Eval loss at step 2000: 3.749028205871582,tokens_seen: 199921040

loss at 4002 1.9140625
Update steps:  15%|███▎                  | 3000/20000 [17:33<1:37:08,  2.92it/s]2025-01-14 13:09:14.733 | INFO     | __main__:main:496 - Performing evaluation at step 3000

loss at 4202 1.8984375

loss at 4402 1.890625

loss at 4602 1.8359375

loss at 4802 1.8671875
lr 0.0007890133016601822
lr 0.0007890133016601822
Mask overlap ratio: 1.00
Mask Update

loss at 5002 1.828125

loss at 5202 1.8203125

loss at 5402 1.875

loss at 5602 1.828125

loss at 5802 1.8359375
lr 0.0007805328438091513
lr 0.0007805328438091513
Mask overlap ratio: 1.00
Mask Update
2025-01-14 13:09:19.110 | INFO     | __main__:evaluate_model:105 - Loaded validation dataset in 4.38 seconds
2025-01-14 13:09:19.111 | INFO     | __main__:evaluate_model:121 - Eval set prepared in 4.38 seconds
2025-01-14 13:09:39.574 | INFO     | __main__:main:508 - Eval loss at step 3000: 3.599424719810486,tokens_seen: 299932842

loss at 6002 1.828125
Update steps:  20%|████▍                 | 4000/20000 [23:28<1:31:22,  2.92it/s]2025-01-14 13:15:10.188 | INFO     | __main__:main:496 - Performing evaluation at step 4000

loss at 6202 1.7734375

loss at 6402 1.796875

loss at 6602 1.859375

loss at 6802 1.796875
lr 0.0007697262013391427
lr 0.0007697262013391427
Mask overlap ratio: 1.00
Mask Update

loss at 7002 1.8203125

loss at 7202 1.7890625

loss at 7402 1.7890625

loss at 7602 1.8046875

loss at 7802 1.75
lr 0.0007566671945760182
lr 0.0007566671945760182
Mask overlap ratio: 1.00
Mask Update
2025-01-14 13:15:13.783 | INFO     | __main__:evaluate_model:105 - Loaded validation dataset in 3.59 seconds
2025-01-14 13:15:13.785 | INFO     | __main__:evaluate_model:121 - Eval set prepared in 3.60 seconds
2025-01-14 13:15:34.043 | INFO     | __main__:main:508 - Eval loss at step 4000: 3.505752444267273,tokens_seen: 399842458

loss at 8002 1.7265625
Update steps:  25%|█████▍                | 4982/20000 [29:16<1:22:36,  3.03it/s]

loss at 8202 1.7734375

loss at 8402 1.765625

loss at 8602 1.765625

loss at 8802 1.7421875
lr 0.0007414450297776283
lr 0.0007414450297776283
Mask overlap ratio: 1.00
Mask Update

loss at 9002 1.71875

loss at 9202 1.7109375

loss at 9402 1.7578125

loss at 9602 1.75

loss at 9802 1.734375
