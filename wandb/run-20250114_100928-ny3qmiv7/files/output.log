[32m2025-01-14 10:09:28.741[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m209[0m - [1mUsing dist with rank 0 (only rank 0 will log)[0m
[32m2025-01-14 10:09:28.741[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m210[0m - [1m****************************************[0m
[32m2025-01-14 10:09:28.742[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m211[0m - [1mStarting training with the arguments[0m
[32m2025-01-14 10:09:28.742[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m213[0m - [1mmodel_config                   configs/llama_60m.json[0m
model_config                   configs/llama_60m.json
[32m2025-01-14 10:09:28.742[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m213[0m - [1muse_hf_model                   False[0m
use_hf_model                   False
[32m2025-01-14 10:09:28.742[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m213[0m - [1mcontinue_from                  None[0m
continue_from                  None
[32m2025-01-14 10:09:28.742[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m213[0m - [1mbatch_size                     128[0m
batch_size                     128
[32m2025-01-14 10:09:28.742[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m213[0m - [1mgradient_accumulation          4[0m
gradient_accumulation          4
[32m2025-01-14 10:09:28.742[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m213[0m - [1mtotal_batch_size               512[0m
total_batch_size               512
[32m2025-01-14 10:09:28.742[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m213[0m - [1mmax_length                     256[0m
max_length                     256
[32m2025-01-14 10:09:28.742[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m213[0m - [1moptimizer                      SPAM[0m
optimizer                      SPAM
[32m2025-01-14 10:09:28.743[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m213[0m - [1mlr                             0.004[0m
lr                             0.004
[32m2025-01-14 10:09:28.743[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m213[0m - [1mscheduler                      cosine[0m
scheduler                      cosine
[32m2025-01-14 10:09:28.743[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m213[0m - [1mmin_lr_ratio                   0.1[0m
min_lr_ratio                   0.1
[32m2025-01-14 10:09:28.743[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m213[0m - [1mactivation_checkpointing       False[0m
activation_checkpointing       False
[32m2025-01-14 10:09:28.743[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m213[0m - [1mweight_decay                   0.0[0m
weight_decay                   0.0
[32m2025-01-14 10:09:28.743[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m213[0m - [1mwarmup_steps                   1000[0m
warmup_steps                   1000
[32m2025-01-14 10:09:28.743[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m213[0m - [1meval_every                     1000[0m
eval_every                     1000
[32m2025-01-14 10:09:28.743[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m213[0m - [1mnum_training_steps             20000[0m
num_training_steps             20000
[32m2025-01-14 10:09:28.743[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m213[0m - [1mmax_train_tokens               None[0m
max_train_tokens               None
[32m2025-01-14 10:09:28.743[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m213[0m - [1msave_every                     2000[0m
save_every                     2000
[32m2025-01-14 10:09:28.743[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m213[0m - [1msave_dir                       /scratch-shared/HTJ2/checkpoints/new0_9390469[0m
save_dir                       /scratch-shared/HTJ2/checkpoints/new0_9390469
[32m2025-01-14 10:09:28.743[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m213[0m - [1mtags                           None[0m
tags                           None
[32m2025-01-14 10:09:28.744[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m213[0m - [1mdtype                          bfloat16[0m
dtype                          bfloat16
[32m2025-01-14 10:09:28.744[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m213[0m - [1mworkers                        8[0m
workers                        8
[32m2025-01-14 10:09:28.744[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m213[0m - [1mseed                           0[0m
seed                           0
[32m2025-01-14 10:09:28.744[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m213[0m - [1mname                           test[0m
name                           test
[32m2025-01-14 10:09:28.744[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m213[0m - [1mmodel_type                     llama[0m
model_type                     llama
[32m2025-01-14 10:09:28.744[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m213[0m - [1mgrad_clipping                  0.0[0m
grad_clipping                  0.0
[32m2025-01-14 10:09:28.744[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m213[0m - [1mbeta1                          0.0[0m
beta1                          0.0
[32m2025-01-14 10:09:28.744[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m213[0m - [1mgalore_scale                   1.0[0m
galore_scale                   1.0
[32m2025-01-14 10:09:28.744[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m213[0m - [1msingle_gpu                     False[0m
single_gpu                     False
[32m2025-01-14 10:09:28.744[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m213[0m - [1mload_local                     False[0m
load_local                     False
[32m2025-01-14 10:09:28.744[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m213[0m - [1mwarmup_epoch                   150[0m
warmup_epoch                   150
[32m2025-01-14 10:09:28.744[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m213[0m - [1mthreshold                      5000.0[0m
threshold                      5000.0
[32m2025-01-14 10:09:28.745[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m213[0m - [1mgrad_accu_steps                20[0m
grad_accu_steps                20
[32m2025-01-14 10:09:28.745[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m213[0m - [1mdensity                        1.0[0m
density                        1.0
[32m2025-01-14 10:09:28.745[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m213[0m - [1mupdate_gap                     500[0m
update_gap                     500
[32m2025-01-14 10:09:28.745[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m215[0m - [1m****************************************[0m
Resolving data files: 100%|██████████████████████████████████████████████████████████████████████| 1024/1024 [00:02<00:00, 413.67it/s]
Resolving data files: 100%|███████████████████████████████████████████████████████████████████| 1024/1024 [00:00<00:00, 465125.33it/s]
[32m2025-01-14 10:09:36.725[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m228[0m - [1mShuffling data with seed 42[0m
/home/huangti/miniconda3/envs/spam/lib/python3.11/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[34m[1mwandb[0m: [33mWARNING[0m Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
Update steps:   0%|                                   | 0/20000 [00:00<?, ?it/s][32m2025-01-14 10:09:38.165[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m345[0m - [1m
LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32000, 512, padding_idx=31999)
    (layers): ModuleList(
      (0-7): 8 x LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=512, out_features=512, bias=False)
          (k_proj): Linear(in_features=512, out_features=512, bias=False)
          (v_proj): Linear(in_features=512, out_features=512, bias=False)
          (o_proj): Linear(in_features=512, out_features=512, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=512, out_features=1376, bias=False)
          (down_proj): Linear(in_features=1376, out_features=512, bias=False)
          (up_proj): Linear(in_features=512, out_features=1376, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=512, out_features=32000, bias=False)
)
[0m
[32m2025-01-14 10:09:38.166[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m346[0m - [1mTotal params: 58.07M[0m
[32m2025-01-14 10:09:38.166[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m347[0m - [1mTrainable params: 58.07M[0m
[32m2025-01-14 10:09:38.166[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m350[0m - [1mSaving model to /scratch-shared/HTJ2/checkpoints/new0_9390469 every 2000 update steps[0m
/gpfs/home2/huangti/SPAM_v2/galore_torch/SPAM.py:104: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use `torch.optim.AdamW` instead, or set `no_deprecation_warning=True` to disable this warning.
  warnings.warn(
density 1.0
/home/huangti/miniconda3/envs/spam/lib/python3.11/site-packages/torch/autograd/graph.py:825: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at /opt/conda/conda-bld/pytorch_1728945379270/work/aten/src/ATen/native/cudnn/MHA.cpp:674.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass

loss at 4 2.625
Update steps:   2%|▌                      | 500/20000 [02:56<1:56:02,  2.80it/s]/home/huangti/miniconda3/envs/spam/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:224: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate

loss at 404 1.734375

loss at 804 1.5

loss at 1204 1.3359375

loss at 1604 1.2421875
lr 0.001992
lr 0.001992
Mask overlap ratio: 1.00
Mask Update
  warnings.warn(
/home/huangti/miniconda3/envs/spam/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

loss at 2004 1.15625
Update steps:   5%|█                     | 1000/20000 [05:48<1:50:53,  2.86it/s][32m2025-01-14 10:15:26.450[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m551[0m - [1mPerforming evaluation at step 1000[0m

loss at 2404 1.8125

loss at 2804 1.3125

loss at 3204 1.265625

loss at 3604 1.1875
lr 0.003992
lr 0.003992
Mask overlap ratio: 1.00
Mask Update
Resolving data files: 100%|██████████████████████████████████████████████████████████████████████| 1024/1024 [00:01<00:00, 545.55it/s]
Resolving data files: 100%|███████████████████████████████████████████████████████████████████| 1024/1024 [00:00<00:00, 578680.58it/s]
[32m2025-01-14 10:15:31.689[0m | [1mINFO    [0m | [36m__main__[0m:[36mevaluate_model[0m:[36m105[0m - [1mLoaded validation dataset in 5.24 seconds[0m
[32m2025-01-14 10:15:31.690[0m | [1mINFO    [0m | [36m__main__[0m:[36mevaluate_model[0m:[36m121[0m - [1mEval set prepared in 5.24 seconds[0m
[32m2025-01-14 10:16:06.092[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m563[0m - [1mEval loss at step 1000: 4.565617561340332,tokens_seen: 99979486[0m

loss at 4004 1.1328125
Update steps:  10%|██▏                   | 2000/20000 [12:11<1:44:51,  2.86it/s][32m2025-01-14 10:21:50.134[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m551[0m - [1mPerforming evaluation at step 2000[0m

loss at 4404 1.140625

loss at 4804 1.15625

loss at 5204 1.125

loss at 5604 1.1015625
lr 0.0039939011446085354
lr 0.0039939011446085354
Mask overlap ratio: 1.00
Mask Update

loss at 6004 1.09375

loss at 6404 1.0859375

loss at 6804 1.234375

loss at 7204 1.375

loss at 7604 1.3671875
lr 0.00397554822383021
lr 0.00397554822383021
Mask overlap ratio: 1.00
Mask Update
Resolving data files: 100%|██████████████████████████████████████████████████████████████████████| 1024/1024 [00:02<00:00, 429.51it/s]
Resolving data files: 100%|███████████████████████████████████████████████████████████████████| 1024/1024 [00:00<00:00, 525571.13it/s]
[32m2025-01-14 10:21:55.601[0m | [1mINFO    [0m | [36m__main__[0m:[36mevaluate_model[0m:[36m105[0m - [1mLoaded validation dataset in 5.47 seconds[0m
[32m2025-01-14 10:21:55.602[0m | [1mINFO    [0m | [36m__main__[0m:[36mevaluate_model[0m:[36m121[0m - [1mEval set prepared in 5.47 seconds[0m
[32m2025-01-14 10:22:27.981[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m563[0m - [1mEval loss at step 2000: 5.627026557922363,tokens_seen: 199880081[0m

loss at 8004 1.4140625
Update steps:  15%|███▎                  | 3000/20000 [18:34<1:40:08,  2.83it/s][32m2025-01-14 10:28:12.318[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m551[0m - [1mPerforming evaluation at step 3000[0m

loss at 8404 1.4140625

loss at 8804 1.421875

loss at 9204 1.4296875

loss at 9604 1.453125
lr 0.003945066508300911
lr 0.003945066508300911
Mask overlap ratio: 1.00
Mask Update

loss at 10004 1.40625

loss at 10404 1.421875

loss at 10804 1.421875

loss at 11204 1.4140625

loss at 11604 1.5
lr 0.0039026642190457565
lr 0.0039026642190457565
Mask overlap ratio: 1.00
Mask Update
Resolving data files: 100%|███████████████████████████████████████████████████████████████████| 1024/1024 [00:00<00:00, 496241.17it/s]
Resolving data files: 100%|███████████████████████████████████████████████████████████████████| 1024/1024 [00:00<00:00, 514305.75it/s]
[32m2025-01-14 10:28:14.850[0m | [1mINFO    [0m | [36m__main__[0m:[36mevaluate_model[0m:[36m105[0m - [1mLoaded validation dataset in 2.53 seconds[0m
[32m2025-01-14 10:28:14.852[0m | [1mINFO    [0m | [36m__main__[0m:[36mevaluate_model[0m:[36m121[0m - [1mEval set prepared in 2.53 seconds[0m
[32m2025-01-14 10:28:47.309[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m563[0m - [1mEval loss at step 3000: 6.042160511016846,tokens_seen: 299903185[0m

loss at 12004 1.4921875
Update steps:  20%|████▍                 | 4000/20000 [24:52<1:33:18,  2.86it/s][32m2025-01-14 10:34:31.569[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m551[0m - [1mPerforming evaluation at step 4000[0m

loss at 12404 1.5

loss at 12804 1.5

loss at 13204 1.5

loss at 13604 1.4609375
lr 0.0038486310066957134
lr 0.0038486310066957134
Mask overlap ratio: 1.00
Mask Update

loss at 14004 1.4921875

loss at 14404 1.5390625

loss at 14804 1.5234375

loss at 15204 1.5234375

loss at 15604 1.5078125
lr 0.003783335972880091
lr 0.003783335972880091
Mask overlap ratio: 1.00
Mask Update
Resolving data files: 100%|███████████████████████████████████████████████████████████████████| 1024/1024 [00:00<00:00, 475317.32it/s]
Resolving data files: 100%|████████████████████████████████████████████████████████████████████| 1024/1024 [00:00<00:00, 35034.65it/s]
[32m2025-01-14 10:34:35.433[0m | [1mINFO    [0m | [36m__main__[0m:[36mevaluate_model[0m:[36m105[0m - [1mLoaded validation dataset in 3.86 seconds[0m
[32m2025-01-14 10:34:35.434[0m | [1mINFO    [0m | [36m__main__[0m:[36mevaluate_model[0m:[36m121[0m - [1mEval set prepared in 3.87 seconds[0m
[32m2025-01-14 10:35:07.521[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m563[0m - [1mEval loss at step 4000: 6.208541393280029,tokens_seen: 399934286[0m

loss at 16004 1.5703125
Update steps:  22%|████▊                 | 4348/20000 [27:28<1:29:25,  2.92it/s]Traceback (most recent call last):

loss at 16404 1.6328125

loss at 16804 1.609375

loss at 17204 1.7734375
  File "/gpfs/home2/huangti/SPAM_v2/torchrun_main.py", line 650, in <module>
  File "/gpfs/home2/huangti/SPAM_v2/torchrun_main.py", line 507, in main
    )
  File "/home/huangti/miniconda3/envs/spam/lib/python3.11/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/huangti/miniconda3/envs/spam/lib/python3.11/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/home/huangti/miniconda3/envs/spam/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/home2/huangti/SPAM_v2/galore_torch/SPAM.py", line 268, in step
    grad_full.mul_(scale_factor)
KeyboardInterrupt
[rank0]: Traceback (most recent call last):
[rank0]:   File "/gpfs/home2/huangti/SPAM_v2/torchrun_main.py", line 650, in <module>
[rank0]:   File "/gpfs/home2/huangti/SPAM_v2/torchrun_main.py", line 507, in main
[rank0]:     )
[rank0]: ^^^^^^
[rank0]:   File "/home/huangti/miniconda3/envs/spam/lib/python3.11/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
[rank0]:     return func.__get__(opt, opt.__class__)(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/huangti/miniconda3/envs/spam/lib/python3.11/site-packages/torch/optim/optimizer.py", line 487, in wrapper
[rank0]:     out = func(*args, **kwargs)
[rank0]:           ^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/huangti/miniconda3/envs/spam/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[rank0]:     return func(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/gpfs/home2/huangti/SPAM_v2/galore_torch/SPAM.py", line 268, in step
[rank0]:     grad_full.mul_(scale_factor)
[rank0]: KeyboardInterrupt
