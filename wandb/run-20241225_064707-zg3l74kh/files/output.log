2024-12-25 06:47:08.091 | INFO     | __main__:main:217 - Using dist with rank 0 (only rank 0 will log)
2024-12-25 06:47:08.092 | INFO     | __main__:main:218 - ****************************************
2024-12-25 06:47:08.092 | INFO     | __main__:main:219 - Starting training with the arguments
2024-12-25 06:47:08.092 | INFO     | __main__:main:221 - model_config                   configs/llama_130m.json
model_config                   configs/llama_130m.json
2024-12-25 06:47:08.092 | INFO     | __main__:main:221 - use_hf_model                   False
use_hf_model                   False
2024-12-25 06:47:08.092 | INFO     | __main__:main:221 - continue_from                  None
continue_from                  None
2024-12-25 06:47:08.092 | INFO     | __main__:main:221 - batch_size                     128
batch_size                     128
2024-12-25 06:47:08.092 | INFO     | __main__:main:221 - gradient_accumulation          4
gradient_accumulation          4
2024-12-25 06:47:08.093 | INFO     | __main__:main:221 - total_batch_size               512
total_batch_size               512
2024-12-25 06:47:08.093 | INFO     | __main__:main:221 - max_length                     256
max_length                     256
2024-12-25 06:47:08.093 | INFO     | __main__:main:221 - optimizer                      SPAM
optimizer                      SPAM
2024-12-25 06:47:08.093 | INFO     | __main__:main:221 - lr                             0.0008
lr                             0.0008
2024-12-25 06:47:08.093 | INFO     | __main__:main:221 - scheduler                      cosine
scheduler                      cosine
2024-12-25 06:47:08.093 | INFO     | __main__:main:221 - min_lr_ratio                   0.1
min_lr_ratio                   0.1
2024-12-25 06:47:08.093 | INFO     | __main__:main:221 - activation_checkpointing       False
activation_checkpointing       False
2024-12-25 06:47:08.094 | INFO     | __main__:main:221 - weight_decay                   0.0
weight_decay                   0.0
2024-12-25 06:47:08.094 | INFO     | __main__:main:221 - warmup_steps                   1000
warmup_steps                   1000
2024-12-25 06:47:08.094 | INFO     | __main__:main:221 - eval_every                     1000
eval_every                     1000
2024-12-25 06:47:08.094 | INFO     | __main__:main:221 - num_training_steps             20000
num_training_steps             20000
2024-12-25 06:47:08.094 | INFO     | __main__:main:221 - max_train_tokens               None
max_train_tokens               None
2024-12-25 06:47:08.094 | INFO     | __main__:main:221 - save_every                     2000
save_every                     2000
2024-12-25 06:47:08.094 | INFO     | __main__:main:221 - save_dir                       /scratch-shared/HTJ2/checkpoints/new0_9144441
save_dir                       /scratch-shared/HTJ2/checkpoints/new0_9144441
2024-12-25 06:47:08.094 | INFO     | __main__:main:221 - tags                           None
tags                           None
2024-12-25 06:47:08.095 | INFO     | __main__:main:221 - dtype                          bfloat16
dtype                          bfloat16
2024-12-25 06:47:08.095 | INFO     | __main__:main:221 - workers                        8
workers                        8
2024-12-25 06:47:08.095 | INFO     | __main__:main:221 - seed                           0
seed                           0
2024-12-25 06:47:08.095 | INFO     | __main__:main:221 - name                           test
name                           test
2024-12-25 06:47:08.095 | INFO     | __main__:main:221 - model_type                     llama
model_type                     llama
2024-12-25 06:47:08.095 | INFO     | __main__:main:221 - grad_clipping                  0.0
grad_clipping                  0.0
2024-12-25 06:47:08.096 | INFO     | __main__:main:221 - beta1                          0.0
beta1                          0.0
2024-12-25 06:47:08.096 | INFO     | __main__:main:221 - rank                           0.4305555555555556
rank                           0.4305555555555556
2024-12-25 06:47:08.096 | INFO     | __main__:main:221 - update_proj_gap                500
update_proj_gap                500
2024-12-25 06:47:08.096 | INFO     | __main__:main:221 - galore_scale                   1.0
galore_scale                   1.0
2024-12-25 06:47:08.096 | INFO     | __main__:main:221 - updating_mask_method           interaction
updating_mask_method           interaction
2024-12-25 06:47:08.096 | INFO     | __main__:main:221 - init_mask                      random
init_mask                      random
2024-12-25 06:47:08.096 | INFO     | __main__:main:221 - single_gpu                     False
single_gpu                     False
2024-12-25 06:47:08.096 | INFO     | __main__:main:221 - mask_grad                      False
mask_grad                      False
2024-12-25 06:47:08.096 | INFO     | __main__:main:221 - spike_clip                     True
spike_clip                     True
2024-12-25 06:47:08.097 | INFO     | __main__:main:221 - sampling                       False
sampling                       False
2024-12-25 06:47:08.097 | INFO     | __main__:main:221 - load_local                     False
load_local                     False
2024-12-25 06:47:08.097 | INFO     | __main__:main:221 - m_replace                      False
m_replace                      False
2024-12-25 06:47:08.097 | INFO     | __main__:main:221 - warmup_epoch                   150
warmup_epoch                   150
2024-12-25 06:47:08.097 | INFO     | __main__:main:221 - threshold                      5000.0
threshold                      5000.0
2024-12-25 06:47:08.097 | INFO     | __main__:main:221 - grad_accu_steps                20.0
grad_accu_steps                20.0
2024-12-25 06:47:08.097 | INFO     | __main__:main:223 - ****************************************
2024-12-25 06:47:18.695 | INFO     | __main__:main:236 - Shuffling data with seed 42
/home/huangti/miniconda3/envs/spam/lib/python3.11/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
Update steps:   0%|                                   | 0/20000 [00:00<?, ?it/s]2024-12-25 06:47:21.155 | INFO     | __main__:main:356 -
LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32000, 768, padding_idx=31999)
    (layers): ModuleList(
      (0-11): 12 x LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=768, out_features=768, bias=False)
          (k_proj): Linear(in_features=768, out_features=768, bias=False)
          (v_proj): Linear(in_features=768, out_features=768, bias=False)
          (o_proj): Linear(in_features=768, out_features=768, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=768, out_features=2048, bias=False)
          (down_proj): Linear(in_features=2048, out_features=768, bias=False)
          (up_proj): Linear(in_features=768, out_features=2048, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=768, out_features=32000, bias=False)
)

2024-12-25 06:47:21.156 | INFO     | __main__:main:357 - Total params: 134.11M
2024-12-25 06:47:21.156 | INFO     | __main__:main:358 - Trainable params: 134.11M
2024-12-25 06:47:21.156 | INFO     | __main__:main:361 - Saving model to /scratch-shared/HTJ2/checkpoints/new0_9144441 every 2000 update steps
/gpfs/home2/huangti/SPAM_v2/galore_torch/SPAM.py:68: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
density 0.43055541427047167
/home/huangti/miniconda3/envs/spam/lib/python3.11/site-packages/torch/autograd/graph.py:825: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at /opt/conda/conda-bld/pytorch_1728945379270/work/aten/src/ATen/native/cudnn/MHA.cpp:674.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass

loss at 4 2.625
Update steps:   2%|▌                      | 500/20000 [05:20<4:13:42,  1.28it/s]/home/huangti/miniconda3/envs/spam/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:224: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate

loss at 404 1.8671875

loss at 804 1.796875

loss at 1204 1.59375

loss at 1604 1.4765625
lr 0.00039840000000000003
lr 0.00039840000000000003
Mask overlap ratio: 1.00
Mask Update
  warnings.warn(
/home/huangti/miniconda3/envs/spam/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

loss at 2004 1.375
Update steps:   5%|█                     | 1000/20000 [10:32<4:05:11,  1.29it/s]2024-12-25 06:57:53.832 | INFO     | __main__:main:564 - Performing evaluation at step 1000

loss at 2404 1.4765625

loss at 2804 1.2734375

loss at 3204 1.203125

loss at 3604 1.15625
lr 0.0007984
lr 0.0007984
Mask overlap ratio: 2.32
Mask Update
2024-12-25 06:57:59.868 | INFO     | __main__:evaluate_model:113 - Loaded validation dataset in 6.03 seconds
2024-12-25 06:57:59.869 | INFO     | __main__:evaluate_model:129 - Eval set prepared in 6.04 seconds
2024-12-25 06:58:43.033 | INFO     | __main__:main:576 - Eval loss at step 1000: 4.375234127044678,tokens_seen: 99979486

loss at 4004 1.0859375
Update steps:  10%|██▏                   | 2000/20000 [21:45<3:52:13,  1.29it/s]2024-12-25 07:09:07.754 | INFO     | __main__:main:564 - Performing evaluation at step 2000

loss at 4404 1.0703125

loss at 4804 1.046875

loss at 5204 1.015625

loss at 5604 0.99609375
lr 0.000798780228921707
lr 0.000798780228921707
Mask overlap ratio: 2.32
Mask Update

loss at 6004 0.984375

loss at 6404 0.96875

loss at 6804 0.97265625

loss at 7204 0.984375

loss at 7604 0.9453125
lr 0.0007951096447660421
lr 0.0007951096447660421
Mask overlap ratio: 2.32
Mask Update
2024-12-25 07:09:14.809 | INFO     | __main__:evaluate_model:113 - Loaded validation dataset in 7.05 seconds
2024-12-25 07:09:14.810 | INFO     | __main__:evaluate_model:129 - Eval set prepared in 7.06 seconds
2024-12-25 07:09:55.037 | INFO     | __main__:main:576 - Eval loss at step 2000: 3.7877182960510254,tokens_seen: 199880081

loss at 8004 0.96484375
Update steps:  15%|███▎                  | 3000/20000 [33:00<3:40:14,  1.29it/s]2024-12-25 07:20:21.356 | INFO     | __main__:main:564 - Performing evaluation at step 3000

loss at 8404 0.96484375

loss at 8804 0.95703125

loss at 9204 0.95703125

loss at 9604 0.91796875
lr 0.0007890133016601822
lr 0.0007890133016601822
Mask overlap ratio: 2.32
Mask Update

loss at 10004 0.94140625

loss at 10404 0.89453125

loss at 10804 0.9140625

loss at 11204 0.92578125

loss at 11604 0.89453125
lr 0.0007805328438091513
lr 0.0007805328438091513
Mask overlap ratio: 2.32
Mask Update
2024-12-25 07:20:25.001 | INFO     | __main__:evaluate_model:113 - Loaded validation dataset in 3.64 seconds
2024-12-25 07:20:25.002 | INFO     | __main__:evaluate_model:129 - Eval set prepared in 3.65 seconds
2024-12-25 07:21:05.454 | INFO     | __main__:main:576 - Eval loss at step 3000: 3.6301045417785645,tokens_seen: 299903185

loss at 12004 0.90234375
Update steps:  20%|████▍                 | 4000/20000 [44:10<3:27:18,  1.29it/s]2024-12-25 07:31:32.519 | INFO     | __main__:main:564 - Performing evaluation at step 4000

loss at 12404 0.90234375

loss at 12804 0.89453125

loss at 13204 0.90625

loss at 13604 0.90234375
lr 0.0007697262013391427
lr 0.0007697262013391427
Mask overlap ratio: 2.32
Mask Update

loss at 14004 0.875

loss at 14404 0.90234375

loss at 14804 0.890625

loss at 15204 0.88671875

loss at 15604 0.89453125
lr 0.0007566671945760182
lr 0.0007566671945760182
Mask overlap ratio: 2.32
Mask Update
2024-12-25 07:31:37.009 | INFO     | __main__:evaluate_model:113 - Loaded validation dataset in 4.49 seconds
2024-12-25 07:31:37.011 | INFO     | __main__:evaluate_model:129 - Eval set prepared in 4.49 seconds
2024-12-25 07:32:17.960 | INFO     | __main__:main:576 - Eval loss at step 4000: 3.5279769897460938,tokens_seen: 399934286

loss at 16004 0.90625
Update steps:  25%|█████▌                | 5000/20000 [55:22<3:14:17,  1.29it/s]2024-12-25 07:42:44.212 | INFO     | __main__:main:564 - Performing evaluation at step 5000

loss at 16404 0.8828125

loss at 16804 0.90234375

loss at 17204 0.88671875

loss at 17604 0.87890625
lr 0.0007414450297776283
lr 0.0007414450297776283
Mask overlap ratio: 2.32
Mask Update

loss at 18004 0.85546875

loss at 18404 0.85546875

loss at 18804 0.890625

loss at 19204 0.87109375

loss at 19604 0.87890625
lr 0.0007241636897646183
lr 0.0007241636897646183
Mask overlap ratio: 2.32
Mask Update
2024-12-25 07:42:48.119 | INFO     | __main__:evaluate_model:113 - Loaded validation dataset in 3.91 seconds
2024-12-25 07:42:48.121 | INFO     | __main__:evaluate_model:129 - Eval set prepared in 3.91 seconds
2024-12-25 07:43:28.826 | INFO     | __main__:main:576 - Eval loss at step 5000: 3.457216501235962,tokens_seen: 499986358

loss at 20004 0.8671875
Update steps:  30%|██████              | 6000/20000 [1:06:35<3:01:38,  1.28it/s]2024-12-25 07:53:57.384 | INFO     | __main__:main:564 - Performing evaluation at step 6000

loss at 20404 0.8359375

loss at 20804 0.87890625

loss at 21204 0.875

loss at 21604 0.8671875
lr 0.0007049412236123251
lr 0.0007049412236123251
Mask overlap ratio: 2.32
Mask Update

loss at 22004 0.87890625

loss at 22404 0.875

loss at 22804 0.875

loss at 23204 0.86328125

loss at 23604 0.85546875
lr 0.0006839089402558932
lr 0.0006839089402558932
Mask overlap ratio: 2.32
Mask Update
2024-12-25 07:54:00.704 | INFO     | __main__:evaluate_model:113 - Loaded validation dataset in 3.32 seconds
2024-12-25 07:54:00.705 | INFO     | __main__:evaluate_model:129 - Eval set prepared in 3.32 seconds
2024-12-25 07:54:41.163 | INFO     | __main__:main:576 - Eval loss at step 6000: 3.4041459560394287,tokens_seen: 600040686

loss at 24004 0.875
Update steps:  35%|███████             | 7000/20000 [1:17:46<2:48:45,  1.28it/s]2024-12-25 08:05:07.561 | INFO     | __main__:main:564 - Performing evaluation at step 7000

loss at 24404 0.8671875

loss at 24804 0.87109375

loss at 25204 0.8984375

loss at 25604 0.85546875
lr 0.0006612105115171021
lr 0.0006612105115171021
Mask overlap ratio: 2.32
Mask Update

loss at 26004 0.87109375

loss at 26404 0.82421875

loss at 26804 0.8671875

loss at 27204 0.86328125

loss at 27604 0.82421875
lr 0.0006370009906801411
lr 0.0006370009906801411
Mask overlap ratio: 2.32
Mask Update
2024-12-25 08:05:12.658 | INFO     | __main__:evaluate_model:113 - Loaded validation dataset in 5.10 seconds
2024-12-25 08:05:12.660 | INFO     | __main__:evaluate_model:129 - Eval set prepared in 5.10 seconds
2024-12-25 08:05:53.321 | INFO     | __main__:main:576 - Eval loss at step 7000: 3.3618688583374023,tokens_seen: 700029429

loss at 28004 0.859375
Update steps:  40%|████████            | 8000/20000 [1:28:58<2:35:44,  1.28it/s]2024-12-25 08:16:20.381 | INFO     | __main__:main:564 - Performing evaluation at step 8000

loss at 28404 0.84375

loss at 28804 0.859375

loss at 29204 0.81640625

loss at 29604 0.82421875
lr 0.000611445753320458
lr 0.000611445753320458
Mask overlap ratio: 2.32
Mask Update

loss at 30004 0.81640625

loss at 30404 0.85546875

loss at 30804 0.85546875

loss at 31204 0.85546875

loss at 31604 0.8359375
lr 0.0005847193676218896
lr 0.0005847193676218896
Mask overlap ratio: 2.32
Mask Update
2024-12-25 08:16:25.685 | INFO     | __main__:evaluate_model:113 - Loaded validation dataset in 5.30 seconds
2024-12-25 08:16:25.687 | INFO     | __main__:evaluate_model:129 - Eval set prepared in 5.31 seconds
2024-12-25 08:17:06.604 | INFO     | __main__:main:576 - Eval loss at step 8000: 3.327735424041748,tokens_seen: 800038852

loss at 32004 0.81640625
Update steps:  45%|█████████           | 9000/20000 [1:40:11<2:22:40,  1.29it/s]2024-12-25 08:27:32.897 | INFO     | __main__:main:564 - Performing evaluation at step 9000

loss at 32404 0.8515625

loss at 32804 0.85546875

loss at 33204 0.84375

loss at 33604 0.83203125
lr 0.0005570044018989555
lr 0.0005570044018989555
Mask overlap ratio: 2.32
Mask Update

loss at 34004 0.8125

loss at 34404 0.84765625

loss at 34804 0.84375

loss at 35204 0.79296875

loss at 35604 0.84765625
lr 0.0005284901774701486
lr 0.0005284901774701486
Mask overlap ratio: 2.32
Mask Update
2024-12-25 08:27:36.017 | INFO     | __main__:evaluate_model:113 - Loaded validation dataset in 3.12 seconds
2024-12-25 08:27:36.019 | INFO     | __main__:evaluate_model:129 - Eval set prepared in 3.12 seconds
2024-12-25 08:28:16.827 | INFO     | __main__:main:576 - Eval loss at step 9000: 3.2976934909820557,tokens_seen: 899981426

loss at 36004 0.8046875
Update steps:  50%|█████████▌         | 10000/20000 [1:51:22<2:10:01,  1.28it/s]2024-12-25 08:38:44.066 | INFO     | __main__:main:564 - Performing evaluation at step 10000

loss at 36404 0.84375

loss at 36804 0.84765625

loss at 37204 0.8125

loss at 37604 0.8125
lr 0.000499371475401361
lr 0.000499371475401361
Mask overlap ratio: 2.32
Mask Update

loss at 38004 0.81640625

loss at 38404 0.828125

loss at 38804 0.7890625

loss at 39204 0.82421875

loss at 39604 0.828125
lr 0.00046984720595369834
lr 0.00046984720595369834
Mask overlap ratio: 2.32
Mask Update
2024-12-25 08:38:48.534 | INFO     | __main__:evaluate_model:113 - Loaded validation dataset in 4.47 seconds
2024-12-25 08:38:48.536 | INFO     | __main__:evaluate_model:129 - Eval set prepared in 4.47 seconds
2024-12-25 08:39:29.397 | INFO     | __main__:main:576 - Eval loss at step 10000: 3.270768404006958,tokens_seen: 999938455

loss at 40004 0.82421875
Update steps:  55%|██████████▍        | 11000/20000 [2:02:34<1:57:05,  1.28it/s]2024-12-25 08:49:55.800 | INFO     | __main__:main:564 - Performing evaluation at step 11000

loss at 40404 0.828125

loss at 40804 0.84375

loss at 41204 0.8203125

loss at 41604 0.8203125
lr 0.000440119049824703
lr 0.000440119049824703
Mask overlap ratio: 2.32
Mask Update

loss at 42004 0.81640625

loss at 42404 0.80859375

loss at 42804 0.8125

loss at 43204 0.80078125

loss at 43604 0.82421875
lr 0.00041039008046469015
lr 0.00041039008046469015
Mask overlap ratio: 2.32
Mask Update
2024-12-25 08:49:59.344 | INFO     | __main__:evaluate_model:113 - Loaded validation dataset in 3.54 seconds
2024-12-25 08:49:59.346 | INFO     | __main__:evaluate_model:129 - Eval set prepared in 3.54 seconds
2024-12-25 08:50:39.848 | INFO     | __main__:main:576 - Eval loss at step 11000: 3.2467269897460938,tokens_seen: 1099986729

loss at 44004 0.80859375
Update steps:  60%|███████████▍       | 12000/20000 [2:13:45<1:44:05,  1.28it/s]2024-12-25 09:01:07.126 | INFO     | __main__:main:564 - Performing evaluation at step 12000

loss at 44404 0.79296875

loss at 44804 0.82421875

loss at 45204 0.82421875

loss at 45604 0.8359375
lr 0.00038086337687916693
lr 0.00038086337687916693
Mask overlap ratio: 2.32
Mask Update

loss at 46004 0.84375

loss at 46404 0.8203125

loss at 46804 0.82421875

loss at 47204 0.79296875

loss at 47604 0.8203125
lr 0.00035174063639330575
lr 0.00035174063639330575
Mask overlap ratio: 2.32
Mask Update
2024-12-25 09:01:11.084 | INFO     | __main__:evaluate_model:113 - Loaded validation dataset in 3.96 seconds
2024-12-25 09:01:11.085 | INFO     | __main__:evaluate_model:129 - Eval set prepared in 3.96 seconds
2024-12-25 09:01:52.779 | INFO     | __main__:main:576 - Eval loss at step 12000: 3.2246339321136475,tokens_seen: 1199964331

loss at 48004 0.79296875
Update steps:  65%|████████████▎      | 13000/20000 [2:24:57<1:30:44,  1.29it/s]2024-12-25 09:12:19.153 | INFO     | __main__:main:564 - Performing evaluation at step 13000

loss at 48404 0.8203125

loss at 48804 0.8046875

loss at 49204 0.8046875

loss at 49604 0.8203125
lr 0.0003232207968546957
lr 0.0003232207968546957
Mask overlap ratio: 2.32
Mask Update

loss at 50004 0.82421875

loss at 50404 0.796875

loss at 50804 0.79296875

loss at 51204 0.8203125

loss at 51604 0.78125
lr 0.00029549867768612244
lr 0.00029549867768612244
Mask overlap ratio: 2.32
Mask Update
2024-12-25 09:12:22.934 | INFO     | __main__:evaluate_model:113 - Loaded validation dataset in 3.78 seconds
2024-12-25 09:12:22.935 | INFO     | __main__:evaluate_model:129 - Eval set prepared in 3.78 seconds
2024-12-25 09:13:03.328 | INFO     | __main__:main:576 - Eval loss at step 13000: 3.2075281143188477,tokens_seen: 1300025548

loss at 52004 0.7734375
Update steps:  70%|█████████████▎     | 14000/20000 [2:36:08<1:17:51,  1.28it/s]2024-12-25 09:23:30.496 | INFO     | __main__:main:564 - Performing evaluation at step 14000

loss at 52404 0.80859375

loss at 52804 0.80078125

loss at 53204 0.8203125

loss at 53604 0.79296875
lr 0.00026876364907136603
lr 0.00026876364907136603
Mask overlap ratio: 2.32
Mask Update

loss at 54004 0.796875

loss at 54404 0.7890625

loss at 54804 0.796875

loss at 55204 0.80078125

loss at 55604 0.81640625
lr 0.00024319833836482783
lr 0.00024319833836482783
Mask overlap ratio: 2.32
Mask Update
2024-12-25 09:23:34.298 | INFO     | __main__:evaluate_model:113 - Loaded validation dataset in 3.80 seconds
2024-12-25 09:23:34.300 | INFO     | __main__:evaluate_model:129 - Eval set prepared in 3.80 seconds
2024-12-25 09:24:15.021 | INFO     | __main__:main:576 - Eval loss at step 14000: 3.192253828048706,tokens_seen: 1400046797

loss at 56004 0.77734375
Update steps:  75%|██████████████▎    | 15000/20000 [2:47:20<1:04:48,  1.29it/s]2024-12-25 09:34:41.405 | INFO     | __main__:main:564 - Performing evaluation at step 15000

loss at 56404 0.7734375

loss at 56804 0.81640625

loss at 57204 0.8046875

loss at 57604 0.81640625
lr 0.0002189773825615175
lr 0.0002189773825615175
Mask overlap ratio: 2.32
Mask Update

loss at 58004 0.828125

loss at 58404 0.76953125

loss at 58804 0.78125

loss at 59204 0.77734375

loss at 59604 0.7734375
lr 0.00019626623534929776
lr 0.00019626623534929776
Mask overlap ratio: 2.32
Mask Update
2024-12-25 09:34:44.940 | INFO     | __main__:evaluate_model:113 - Loaded validation dataset in 3.53 seconds
2024-12-25 09:34:44.942 | INFO     | __main__:evaluate_model:129 - Eval set prepared in 3.54 seconds
2024-12-25 09:35:26.607 | INFO     | __main__:main:576 - Eval loss at step 15000: 3.18196702003479,tokens_seen: 1500063597

loss at 60004 0.78125
Update steps:  80%|████████████████▊    | 16000/20000 [2:58:31<51:51,  1.29it/s]2024-12-25 09:45:53.734 | INFO     | __main__:main:564 - Performing evaluation at step 16000

loss at 60404 0.79296875

loss at 60804 0.796875

loss at 61204 0.81640625

loss at 61604 0.81640625
lr 0.00017522003689242823
lr 0.00017522003689242823
Mask overlap ratio: 2.32
Mask Update

loss at 62004 0.828125

loss at 62404 0.80078125

loss at 62804 0.8046875

loss at 63204 0.7890625

loss at 63604 0.80078125
lr 0.00015598255406693392
lr 0.00015598255406693392
Mask overlap ratio: 2.32
Mask Update
2024-12-25 09:45:57.718 | INFO     | __main__:evaluate_model:113 - Loaded validation dataset in 3.98 seconds
2024-12-25 09:45:57.719 | INFO     | __main__:evaluate_model:129 - Eval set prepared in 3.99 seconds
2024-12-25 09:46:38.323 | INFO     | __main__:main:576 - Eval loss at step 16000: 3.173433780670166,tokens_seen: 1600154706

loss at 64004 0.80859375
Update steps:  85%|█████████████████▊   | 17000/20000 [3:09:43<39:07,  1.28it/s]2024-12-25 09:57:04.715 | INFO     | __main__:main:564 - Performing evaluation at step 17000

loss at 64404 0.7890625

loss at 64804 0.7890625

loss at 65204 0.8046875

loss at 65604 0.80859375
lr 0.000138685198387064
lr 0.000138685198387064
Mask overlap ratio: 2.32
Mask Update

loss at 66004 0.78125

loss at 66404 0.796875

loss at 66804 0.7890625

loss at 67204 0.77734375

loss at 67604 0.76953125
lr 0.00012344612833139982
lr 0.00012344612833139982
Mask overlap ratio: 2.32
Mask Update
2024-12-25 09:57:08.934 | INFO     | __main__:evaluate_model:113 - Loaded validation dataset in 4.22 seconds
2024-12-25 09:57:08.936 | INFO     | __main__:evaluate_model:129 - Eval set prepared in 4.22 seconds
2024-12-25 09:57:49.348 | INFO     | __main__:main:576 - Eval loss at step 17000: 3.167900562286377,tokens_seen: 1700164802

loss at 68004 0.8046875
Update steps:  90%|██████████████████▉  | 18000/20000 [3:20:52<25:51,  1.29it/s]2024-12-25 10:08:14.251 | INFO     | __main__:main:564 - Performing evaluation at step 18000

loss at 68404 0.8046875

loss at 68804 0.78125

loss at 69204 0.77734375

loss at 69604 0.79296875
lr 0.00011036944220063523
lr 0.00011036944220063523
Mask overlap ratio: 2.32
Mask Update

loss at 70004 0.8046875

loss at 70404 0.80078125

loss at 70804 0.80859375

loss at 71204 0.78125

loss at 71604 0.8125
lr 9.954446702062876e-05
lr 9.954446702062876e-05
Mask overlap ratio: 2.32
Mask Update
2024-12-25 10:08:18.418 | INFO     | __main__:evaluate_model:113 - Loaded validation dataset in 4.17 seconds
2024-12-25 10:08:18.420 | INFO     | __main__:evaluate_model:129 - Eval set prepared in 4.17 seconds
2024-12-25 10:08:59.018 | INFO     | __main__:main:576 - Eval loss at step 18000: 3.1639652252197266,tokens_seen: 1800161308

loss at 72004 0.8125
Update steps:  95%|███████████████████▉ | 19000/20000 [3:32:01<12:55,  1.29it/s]2024-12-25 10:19:22.966 | INFO     | __main__:main:564 - Performing evaluation at step 19000

loss at 72404 0.81640625

loss at 72804 0.8125

loss at 73204 0.7890625

loss at 73604 0.7734375
lr 9.104514834824031e-05
lr 9.104514834824031e-05
Mask overlap ratio: 2.32
Mask Update

loss at 74004 0.79296875

loss at 74404 0.7890625

loss at 74804 0.78125

loss at 75204 0.8125

loss at 75604 0.7890625
lr 8.492954514819799e-05
lr 8.492954514819799e-05
Mask overlap ratio: 2.32
Mask Update
2024-12-25 10:19:26.584 | INFO     | __main__:evaluate_model:113 - Loaded validation dataset in 3.62 seconds
2024-12-25 10:19:26.586 | INFO     | __main__:evaluate_model:129 - Eval set prepared in 3.62 seconds
2024-12-25 10:20:07.441 | INFO     | __main__:main:576 - Eval loss at step 19000: 3.1610429286956787,tokens_seen: 1900163851

loss at 76004 0.8046875
Update steps: 100%|█████████████████████| 20000/20000 [3:43:10<00:00,  1.29it/s]2024-12-25 10:30:32.102 | INFO     | __main__:main:564 - Performing evaluation at step 20000

loss at 76404 0.80078125

loss at 76804 0.8125

loss at 77204 0.8125

loss at 77604 0.82421875
lr 8.123943319149816e-05
lr 8.123943319149816e-05
Mask overlap ratio: 2.32
Mask Update

loss at 78004 0.765625

loss at 78404 0.77734375

loss at 78804 0.78515625

loss at 79204 0.796875

loss at 79604 0.78515625
lr 8.000001968452938e-05
lr 8.000001968452938e-05
Mask overlap ratio: 2.32
Mask Update
2024-12-25 10:30:36.386 | INFO     | __main__:evaluate_model:113 - Loaded validation dataset in 4.28 seconds
2024-12-25 10:30:36.387 | INFO     | __main__:evaluate_model:129 - Eval set prepared in 4.28 seconds
2024-12-25 10:31:17.639 | INFO     | __main__:main:576 - Eval loss at step 20000: 3.1595232486724854,tokens_seen: 2000123948

loss at 80004 0.78125
Update steps: 20001it [3:43:57, 14.62s/it]                                      2024-12-25 10:31:18.252 | INFO     | __main__:main:493 - Reached max number of update steps (f20000). Stopping training.
Rank 0 stopping training.
2024-12-25 10:31:18.627 | INFO     | __main__:main:603 - Training finished
Update steps: 20001it [3:43:57,  1.49it/s]
2024-12-25 10:31:18.628 | INFO     | __main__:main:608 - Saving model and optimizer to /scratch-shared/HTJ2/checkpoints/new0_9144441/model_20001, update step 20001
2024-12-25 10:31:19.330 | INFO     | __main__:main:635 - Running final evaluation
2024-12-25 10:31:25.678 | INFO     | __main__:evaluate_model:113 - Loaded validation dataset in 6.17 seconds
2024-12-25 10:31:25.680 | INFO     | __main__:evaluate_model:129 - Eval set prepared in 6.17 seconds
2024-12-25 10:32:06.633 | INFO     | __main__:main:653 - Final eval loss: 3.1595232486724854
2024-12-25 10:32:06.634 | INFO     | __main__:main:655 - Script finished successfully
Rank 0 finished successfully
perplexity 23.55936129444491
