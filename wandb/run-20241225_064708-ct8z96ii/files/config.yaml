_wandb:
    value:
        cli_version: 0.18.5
        m: []
        python_version: 3.11.10
        t:
            "1":
                - 1
                - 11
                - 49
                - 51
                - 55
            "2":
                - 1
                - 11
                - 49
                - 51
                - 55
            "3":
                - 3
                - 23
                - 55
                - 61
            "4": 3.11.10
            "5": 0.18.5
            "6": 4.31.0
            "8":
                - 5
            "12": 0.18.5
            "13": linux-x86_64
activation_checkpointing:
    value: false
batch_size:
    value: 128
beta1:
    value: 0
continue_from:
    value: null
dataset:
    value: c4
device:
    value: cuda:0
dtype:
    value: bfloat16
eval_every:
    value: 1000
galore_scale:
    value: 1
grad_accu_steps:
    value: 20
grad_clipping:
    value: 0
gradient_accumulation:
    value: 2
init_mask:
    value: random
load_local:
    value: false
m_replace:
    value: false
mask_grad:
    value: false
max_length:
    value: 256
max_lr:
    value: 0.002
max_train_tokens:
    value: null
min_lr_ratio:
    value: 0.1
model:
    value:
        _name_or_path: configs/llama_350m.json
        add_cross_attention: false
        architectures:
            - LLaMAForCausalLM
        bad_words_ids: null
        begin_suppress_tokens: null
        bos_token_id: 0
        chunk_size_feed_forward: 0
        cross_attention_hidden_size: null
        decoder_start_token_id: null
        diversity_penalty: 0
        do_sample: false
        early_stopping: false
        encoder_no_repeat_ngram_size: 0
        eos_token_id: 1
        exponential_decay_length_penalty: null
        finetuning_task: null
        forced_bos_token_id: null
        forced_eos_token_id: null
        hidden_act: silu
        hidden_size: 1024
        id2label:
            "0": LABEL_0
            "1": LABEL_1
        initializer_range: 0.02
        intermediate_size: 2736
        is_decoder: false
        is_encoder_decoder: false
        label2id:
            LABEL_0: 0
            LABEL_1: 1
        length_penalty: 1
        max_length: 20
        max_position_embeddings: 2048
        max_sequence_length: 1024
        min_length: 0
        model_type: llama
        no_repeat_ngram_size: 0
        num_attention_heads: 16
        num_beam_groups: 1
        num_beams: 1
        num_hidden_layers: 24
        num_key_value_heads: 16
        num_return_sequences: 1
        output_attentions: false
        output_hidden_states: false
        output_scores: false
        pad_token_id: -1
        prefix: null
        pretraining_tp: 1
        problem_type: null
        remove_invalid_values: false
        repetition_penalty: 1
        return_dict: true
        return_dict_in_generate: false
        rms_norm_eps: 1e-06
        rope_scaling: null
        sep_token_id: null
        suppress_tokens: null
        task_specific_params: null
        temperature: 1
        tf_legacy_loss: false
        tie_encoder_decoder: false
        tie_word_embeddings: false
        tokenizer_class: null
        top_k: 50
        top_p: 1
        torch_dtype: null
        torchscript: false
        transformers_version: 4.31.0
        typical_p: 1
        use_bfloat16: false
        use_cache: true
        vocab_size: 32000
model_config:
    value: configs/llama_350m.json
model_type:
    value: llama
name:
    value: test
num_training_steps:
    value: 20000
optimizer:
    value: SPAM
rank:
    value: 0.3225388601036269
sampling:
    value: false
save_dir:
    value: /scratch-shared/HTJ2/checkpoints/new0_9144442
save_every:
    value: 2000
scheduler:
    value: cosine
seed:
    value: 0
single_gpu:
    value: false
spike_clip:
    value: true
tags:
    value: null
threshold:
    value: 5000
total_batch_size:
    value: 512
total_params_M:
    value: 367.96928
update_proj_gap:
    value: 500
updating_mask_method:
    value: interaction
use_hf_model:
    value: false
warmup_epoch:
    value: 150
warmup_steps:
    value: 1000
weight_decay:
    value: 0
workers:
    value: 8
world_size:
    value: 2
