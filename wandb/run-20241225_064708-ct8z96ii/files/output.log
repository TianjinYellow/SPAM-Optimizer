2024-12-25 06:47:09.472 | INFO     | __main__:main:217 - Using dist with rank 0 (only rank 0 will log)
2024-12-25 06:47:09.472 | INFO     | __main__:main:218 - ****************************************
2024-12-25 06:47:09.472 | INFO     | __main__:main:219 - Starting training with the arguments
2024-12-25 06:47:09.472 | INFO     | __main__:main:221 - model_config                   configs/llama_350m.json
model_config                   configs/llama_350m.json
2024-12-25 06:47:09.473 | INFO     | __main__:main:221 - use_hf_model                   False
use_hf_model                   False
2024-12-25 06:47:09.473 | INFO     | __main__:main:221 - continue_from                  None
continue_from                  None
2024-12-25 06:47:09.473 | INFO     | __main__:main:221 - batch_size                     128
batch_size                     128
2024-12-25 06:47:09.473 | INFO     | __main__:main:221 - gradient_accumulation          2
gradient_accumulation          2
2024-12-25 06:47:09.473 | INFO     | __main__:main:221 - total_batch_size               512
total_batch_size               512
2024-12-25 06:47:09.474 | INFO     | __main__:main:221 - max_length                     256
max_length                     256
2024-12-25 06:47:09.474 | INFO     | __main__:main:221 - optimizer                      SPAM
optimizer                      SPAM
2024-12-25 06:47:09.474 | INFO     | __main__:main:221 - lr                             0.002
lr                             0.002
2024-12-25 06:47:09.474 | INFO     | __main__:main:221 - scheduler                      cosine
scheduler                      cosine
2024-12-25 06:47:09.474 | INFO     | __main__:main:221 - min_lr_ratio                   0.1
min_lr_ratio                   0.1
2024-12-25 06:47:09.474 | INFO     | __main__:main:221 - activation_checkpointing       False
activation_checkpointing       False
2024-12-25 06:47:09.474 | INFO     | __main__:main:221 - weight_decay                   0.0
weight_decay                   0.0
2024-12-25 06:47:09.474 | INFO     | __main__:main:221 - warmup_steps                   1000
warmup_steps                   1000
2024-12-25 06:47:09.474 | INFO     | __main__:main:221 - eval_every                     1000
eval_every                     1000
2024-12-25 06:47:09.475 | INFO     | __main__:main:221 - num_training_steps             20000
num_training_steps             20000
2024-12-25 06:47:09.475 | INFO     | __main__:main:221 - max_train_tokens               None
max_train_tokens               None
2024-12-25 06:47:09.475 | INFO     | __main__:main:221 - save_every                     2000
save_every                     2000
2024-12-25 06:47:09.475 | INFO     | __main__:main:221 - save_dir                       /scratch-shared/HTJ2/checkpoints/new0_9144442
save_dir                       /scratch-shared/HTJ2/checkpoints/new0_9144442
2024-12-25 06:47:09.475 | INFO     | __main__:main:221 - tags                           None
tags                           None
2024-12-25 06:47:09.475 | INFO     | __main__:main:221 - dtype                          bfloat16
dtype                          bfloat16
2024-12-25 06:47:09.475 | INFO     | __main__:main:221 - workers                        8
workers                        8
2024-12-25 06:47:09.475 | INFO     | __main__:main:221 - seed                           0
seed                           0
2024-12-25 06:47:09.475 | INFO     | __main__:main:221 - name                           test
name                           test
2024-12-25 06:47:09.476 | INFO     | __main__:main:221 - model_type                     llama
model_type                     llama
2024-12-25 06:47:09.476 | INFO     | __main__:main:221 - grad_clipping                  0.0
grad_clipping                  0.0
2024-12-25 06:47:09.476 | INFO     | __main__:main:221 - beta1                          0.0
beta1                          0.0
2024-12-25 06:47:09.476 | INFO     | __main__:main:221 - rank                           0.3225388601036269
rank                           0.3225388601036269
2024-12-25 06:47:09.476 | INFO     | __main__:main:221 - update_proj_gap                500
update_proj_gap                500
2024-12-25 06:47:09.476 | INFO     | __main__:main:221 - galore_scale                   1.0
galore_scale                   1.0
2024-12-25 06:47:09.476 | INFO     | __main__:main:221 - updating_mask_method           interaction
updating_mask_method           interaction
2024-12-25 06:47:09.476 | INFO     | __main__:main:221 - init_mask                      random
init_mask                      random
2024-12-25 06:47:09.476 | INFO     | __main__:main:221 - single_gpu                     False
single_gpu                     False
2024-12-25 06:47:09.477 | INFO     | __main__:main:221 - mask_grad                      False
mask_grad                      False
2024-12-25 06:47:09.477 | INFO     | __main__:main:221 - spike_clip                     True
spike_clip                     True
2024-12-25 06:47:09.477 | INFO     | __main__:main:221 - sampling                       False
sampling                       False
2024-12-25 06:47:09.477 | INFO     | __main__:main:221 - load_local                     False
load_local                     False
2024-12-25 06:47:09.477 | INFO     | __main__:main:221 - m_replace                      False
m_replace                      False
2024-12-25 06:47:09.477 | INFO     | __main__:main:221 - warmup_epoch                   150
warmup_epoch                   150
2024-12-25 06:47:09.477 | INFO     | __main__:main:221 - threshold                      5000.0
threshold                      5000.0
2024-12-25 06:47:09.477 | INFO     | __main__:main:221 - grad_accu_steps                20.0
grad_accu_steps                20.0
2024-12-25 06:47:09.477 | INFO     | __main__:main:223 - ****************************************
2024-12-25 06:47:19.485 | INFO     | __main__:main:236 - Shuffling data with seed 42
/home/huangti/miniconda3/envs/spam/lib/python3.11/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
Update steps:   0%|                                   | 0/20000 [00:00<?, ?it/s]2024-12-25 06:47:24.743 | INFO     | __main__:main:356 -
LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32000, 1024, padding_idx=31999)
    (layers): ModuleList(
      (0-23): 24 x LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=1024, out_features=1024, bias=False)
          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)
          (o_proj): Linear(in_features=1024, out_features=1024, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=1024, out_features=2736, bias=False)
          (down_proj): Linear(in_features=2736, out_features=1024, bias=False)
          (up_proj): Linear(in_features=1024, out_features=2736, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=1024, out_features=32000, bias=False)
)

2024-12-25 06:47:24.747 | INFO     | __main__:main:357 - Total params: 367.97M
2024-12-25 06:47:24.748 | INFO     | __main__:main:358 - Trainable params: 367.97M
2024-12-25 06:47:24.748 | INFO     | __main__:main:361 - Saving model to /scratch-shared/HTJ2/checkpoints/new0_9144442 every 2000 update steps
/gpfs/home2/huangti/SPAM_v2/galore_torch/SPAM.py:68: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
density 0.32253857675857445
/home/huangti/miniconda3/envs/spam/lib/python3.11/site-packages/torch/autograd/graph.py:825: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at /opt/conda/conda-bld/pytorch_1728945379270/work/aten/src/ATen/native/cudnn/MHA.cpp:674.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass

loss at 2 5.28125
Update steps:   2%|▌                      | 500/20000 [06:57<7:13:43,  1.33s/it]/home/huangti/miniconda3/envs/spam/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:224: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate

loss at 202 3.625

loss at 402 3.203125

loss at 602 2.875

loss at 802 2.640625
lr 0.000996
lr 0.000996
Mask overlap ratio: 1.00
Mask Update
  warnings.warn(
/home/huangti/miniconda3/envs/spam/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

loss at 1002 2.453125
Update steps:   5%|█                     | 1000/20000 [13:43<7:00:13,  1.33s/it]2024-12-25 07:01:08.274 | INFO     | __main__:main:564 - Performing evaluation at step 1000

loss at 1202 2.4375

loss at 1402 3.125

loss at 1602 2.875

loss at 1802 2.734375
lr 0.001996
lr 0.001996
Mask overlap ratio: 3.10
Mask Update
2024-12-25 07:01:15.517 | INFO     | __main__:evaluate_model:113 - Loaded validation dataset in 7.24 seconds
2024-12-25 07:01:15.518 | INFO     | __main__:evaluate_model:129 - Eval set prepared in 7.24 seconds
2024-12-25 07:01:54.820 | INFO     | __main__:main:576 - Eval loss at step 1000: 5.190298318862915,tokens_seen: 99962688

loss at 2002 2.59375
Update steps:  10%|██▏                   | 2000/20000 [28:00<6:38:10,  1.33s/it]2024-12-25 07:15:27.632 | INFO     | __main__:main:564 - Performing evaluation at step 2000

loss at 2202 2.578125

loss at 2402 2.4375

loss at 2602 2.40625

loss at 2802 2.375
lr 0.0019969505723042677
lr 0.0019969505723042677
Mask overlap ratio: 3.10
Mask Update

loss at 3002 2.359375

loss at 3202 2.3125

loss at 3402 2.40625

loss at 3602 2.609375

loss at 3802 2.546875
lr 0.001987774111915105
lr 0.001987774111915105
Mask overlap ratio: 3.10
Mask Update
2024-12-25 07:15:34.855 | INFO     | __main__:evaluate_model:113 - Loaded validation dataset in 7.22 seconds
2024-12-25 07:15:34.856 | INFO     | __main__:evaluate_model:129 - Eval set prepared in 7.22 seconds
2024-12-25 07:16:09.425 | INFO     | __main__:main:576 - Eval loss at step 2000: 4.903528928756714,tokens_seen: 199921040

loss at 4002 2.484375
Update steps:  15%|███▎                  | 3000/20000 [42:15<6:13:15,  1.32s/it]2024-12-25 07:29:40.566 | INFO     | __main__:main:564 - Performing evaluation at step 3000

loss at 4202 2.484375

loss at 4402 2.5625

loss at 4602 2.515625

loss at 4802 2.609375
lr 0.0019725332541504553
lr 0.0019725332541504553
Mask overlap ratio: 3.10
Mask Update

loss at 5002 2.5625

loss at 5202 2.65625

loss at 5402 2.796875

loss at 5602 2.84375

loss at 5802 2.796875
lr 0.0019513321095228782
lr 0.0019513321095228782
Mask overlap ratio: 3.10
Mask Update
2024-12-25 07:29:44.921 | INFO     | __main__:evaluate_model:113 - Loaded validation dataset in 4.35 seconds
2024-12-25 07:29:44.923 | INFO     | __main__:evaluate_model:129 - Eval set prepared in 4.36 seconds
2024-12-25 07:30:19.670 | INFO     | __main__:main:576 - Eval loss at step 3000: 5.450559616088867,tokens_seen: 299932842

loss at 6002 2.765625
Update steps:  20%|████▍                 | 4000/20000 [56:26<5:52:17,  1.32s/it]2024-12-25 07:43:53.286 | INFO     | __main__:main:564 - Performing evaluation at step 4000

loss at 6202 2.734375

loss at 6402 2.875

loss at 6602 2.921875

loss at 6802 2.875
lr 0.0019243155033478567
lr 0.0019243155033478567
Mask overlap ratio: 3.10
Mask Update

loss at 7002 2.859375

loss at 7202 2.828125

loss at 7402 2.859375

loss at 7602 3.03125

loss at 7802 2.84375
lr 0.0018916679864400454
lr 0.0018916679864400454
Mask overlap ratio: 3.10
Mask Update
2024-12-25 07:43:57.233 | INFO     | __main__:evaluate_model:113 - Loaded validation dataset in 3.95 seconds
2024-12-25 07:43:57.235 | INFO     | __main__:evaluate_model:129 - Eval set prepared in 3.95 seconds
2024-12-25 07:44:31.437 | INFO     | __main__:main:576 - Eval loss at step 4000: 5.530317068099976,tokens_seen: 399842458

loss at 8002 2.765625
Update steps:  25%|█████               | 5000/20000 [1:10:38<5:30:04,  1.32s/it]2024-12-25 07:58:03.217 | INFO     | __main__:main:564 - Performing evaluation at step 5000

loss at 8202 2.84375

loss at 8402 2.828125

loss at 8602 2.84375

loss at 8802 2.96875
lr 0.0018536125744440706
lr 0.0018536125744440706
Mask overlap ratio: 3.10
Mask Update

loss at 9002 2.828125

loss at 9202 2.890625

loss at 9402 2.890625

loss at 9602 2.9375

loss at 9802 2.828125
lr 0.0018104092244115456
lr 0.0018104092244115456
Mask overlap ratio: 3.10
Mask Update
2024-12-25 07:58:07.482 | INFO     | __main__:evaluate_model:113 - Loaded validation dataset in 4.26 seconds
2024-12-25 07:58:07.484 | INFO     | __main__:evaluate_model:129 - Eval set prepared in 4.27 seconds
2024-12-25 07:58:42.313 | INFO     | __main__:main:576 - Eval loss at step 5000: 5.640625,tokens_seen: 499891938

loss at 10002 2.8125
Update steps:  30%|██████              | 6000/20000 [1:24:49<5:07:31,  1.32s/it]2024-12-25 08:12:16.013 | INFO     | __main__:main:564 - Performing evaluation at step 6000

loss at 10202 2.921875

loss at 10402 2.96875

loss at 10602 2.859375

loss at 10802 2.921875
lr 0.0017623530590308127
lr 0.0017623530590308127
Mask overlap ratio: 3.10
Mask Update

loss at 11002 2.84375

loss at 11202 2.9375

loss at 11402 2.90625

loss at 11602 2.921875

loss at 11802 2.890625
lr 0.0017097723506397332
lr 0.0017097723506397332
Mask overlap ratio: 3.10
Mask Update
2024-12-25 08:12:19.997 | INFO     | __main__:evaluate_model:113 - Loaded validation dataset in 3.98 seconds
2024-12-25 08:12:19.999 | INFO     | __main__:evaluate_model:129 - Eval set prepared in 3.99 seconds
2024-12-25 08:12:53.533 | INFO     | __main__:main:576 - Eval loss at step 6000: 5.7306435108184814,tokens_seen: 599873704

loss at 12002 2.890625
Update steps:  35%|███████             | 7000/20000 [1:39:00<4:45:46,  1.32s/it]2024-12-25 08:26:25.149 | INFO     | __main__:main:564 - Performing evaluation at step 7000

loss at 12202 2.84375

loss at 12402 2.96875

loss at 12602 2.921875

loss at 12802 2.921875
lr 0.001653026278792755
lr 0.001653026278792755
Mask overlap ratio: 3.10
Mask Update

loss at 13002 2.921875

loss at 13202 2.9375

loss at 13402 3.0625

loss at 13602 2.90625

loss at 13802 2.875
lr 0.0015925024767003526
lr 0.0015925024767003526
Mask overlap ratio: 3.10
Mask Update
2024-12-25 08:26:28.727 | INFO     | __main__:evaluate_model:113 - Loaded validation dataset in 3.58 seconds
2024-12-25 08:26:28.729 | INFO     | __main__:evaluate_model:129 - Eval set prepared in 3.58 seconds
2024-12-25 08:27:04.560 | INFO     | __main__:main:576 - Eval loss at step 7000: 5.7061567306518555,tokens_seen: 699878274

loss at 14002 2.84375
Update steps:  40%|████████            | 8000/20000 [1:53:11<4:23:43,  1.32s/it]2024-12-25 08:40:38.195 | INFO     | __main__:main:564 - Performing evaluation at step 8000

loss at 14202 2.921875

loss at 14402 2.875

loss at 14602 2.875

loss at 14802 2.859375
lr 0.0015286143833011451
lr 0.0015286143833011451
Mask overlap ratio: 3.10
Mask Update

loss at 15002 2.8125

loss at 15202 2.90625

loss at 15402 2.84375

loss at 15602 2.84375

loss at 15802 2.9375
lr 0.001461798419054724
lr 0.001461798419054724
Mask overlap ratio: 3.10
Mask Update
2024-12-25 08:40:42.317 | INFO     | __main__:evaluate_model:113 - Loaded validation dataset in 4.12 seconds
2024-12-25 08:40:42.319 | INFO     | __main__:evaluate_model:129 - Eval set prepared in 4.12 seconds
2024-12-25 08:41:16.244 | INFO     | __main__:main:576 - Eval loss at step 8000: 5.62181282043457,tokens_seen: 799804230

loss at 16002 2.828125
Update steps:  45%|█████████           | 9000/20000 [2:07:22<4:01:50,  1.32s/it]2024-12-25 08:54:47.375 | INFO     | __main__:main:564 - Performing evaluation at step 9000

loss at 16202 2.828125

loss at 16402 2.890625

loss at 16602 2.90625

loss at 16802 2.859375
lr 0.0013925110047473886
lr 0.0013925110047473886
Mask overlap ratio: 3.10
Mask Update

loss at 17002 2.859375

loss at 17202 2.84375

loss at 17402 2.828125

loss at 17602 2.8125

loss at 17802 2.78125
lr 0.0013212254436753713
lr 0.0013212254436753713
Mask overlap ratio: 3.10
Mask Update
2024-12-25 08:54:52.244 | INFO     | __main__:evaluate_model:113 - Loaded validation dataset in 4.87 seconds
2024-12-25 08:54:52.245 | INFO     | __main__:evaluate_model:129 - Eval set prepared in 4.87 seconds
2024-12-25 08:55:26.863 | INFO     | __main__:main:576 - Eval loss at step 9000: 5.576880931854248,tokens_seen: 899663440

loss at 18002 2.828125
Update steps:  50%|█████████▌         | 10000/20000 [2:21:33<3:39:48,  1.32s/it]2024-12-25 09:09:00.296 | INFO     | __main__:main:564 - Performing evaluation at step 10000

loss at 18202 2.875

loss at 18402 2.828125

loss at 18602 2.8125

loss at 18802 2.859375
lr 0.0012484286885034027
lr 0.0012484286885034027
Mask overlap ratio: 3.10
Mask Update

loss at 19002 2.765625

loss at 19202 2.78125

loss at 19402 2.796875

loss at 19602 2.8125

loss at 19802 2.765625
lr 0.0011746180148842458
lr 0.0011746180148842458
Mask overlap ratio: 3.10
Mask Update
2024-12-25 09:09:04.119 | INFO     | __main__:evaluate_model:113 - Loaded validation dataset in 3.82 seconds
2024-12-25 09:09:04.121 | INFO     | __main__:evaluate_model:129 - Eval set prepared in 3.82 seconds
2024-12-25 09:09:38.812 | INFO     | __main__:main:576 - Eval loss at step 10000: 5.573693752288818,tokens_seen: 999628698

loss at 20002 2.796875
Update steps:  55%|██████████▍        | 11000/20000 [2:35:45<3:17:52,  1.32s/it]2024-12-25 09:23:10.214 | INFO     | __main__:main:564 - Performing evaluation at step 11000

loss at 20202 2.765625

loss at 20402 2.765625

loss at 20602 2.8125

loss at 20802 2.84375
lr 0.0011002976245617574
lr 0.0011002976245617574
Mask overlap ratio: 3.10
Mask Update

loss at 21002 2.78125

loss at 21202 2.796875

loss at 21402 2.859375

loss at 21602 2.828125

loss at 21802 2.796875
lr 0.0010259752011617254
lr 0.0010259752011617254
Mask overlap ratio: 3.10
Mask Update
2024-12-25 09:23:14.104 | INFO     | __main__:evaluate_model:113 - Loaded validation dataset in 3.89 seconds
2024-12-25 09:23:14.106 | INFO     | __main__:evaluate_model:129 - Eval set prepared in 3.89 seconds
2024-12-25 09:23:49.855 | INFO     | __main__:main:576 - Eval loss at step 11000: 5.54236626625061,tokens_seen: 1099630032

loss at 22002 2.796875
Update steps:  60%|███████████▍       | 12000/20000 [2:49:56<2:55:38,  1.32s/it]2024-12-25 09:37:22.806 | INFO     | __main__:main:564 - Performing evaluation at step 12000

loss at 22202 2.8125

loss at 22402 2.8125

loss at 22602 2.828125

loss at 22802 2.75
lr 0.0009521584421979172
lr 0.0009521584421979172
Mask overlap ratio: 3.10
Mask Update

loss at 23002 2.71875

loss at 23202 2.765625

loss at 23402 2.75

loss at 23602 2.734375

loss at 23802 2.734375
lr 0.0008793515909832644
lr 0.0008793515909832644
Mask overlap ratio: 3.10
Mask Update
2024-12-25 09:37:26.549 | INFO     | __main__:evaluate_model:113 - Loaded validation dataset in 3.74 seconds
2024-12-25 09:37:26.550 | INFO     | __main__:evaluate_model:129 - Eval set prepared in 3.74 seconds
2024-12-25 09:38:00.451 | INFO     | __main__:main:576 - Eval loss at step 12000: 5.4439520835876465,tokens_seen: 1199538724

loss at 24002 2.703125
Update steps:  65%|████████████▎      | 13000/20000 [3:04:06<2:33:52,  1.32s/it]2024-12-25 09:51:31.709 | INFO     | __main__:main:564 - Performing evaluation at step 13000

loss at 24202 2.78125

loss at 24402 2.71875

loss at 24602 2.71875

loss at 24802 2.75
lr 0.0008080519921367393
lr 0.0008080519921367393
Mask overlap ratio: 3.10
Mask Update

loss at 25002 2.6875

loss at 25202 2.6875

loss at 25402 2.734375

loss at 25602 2.703125

loss at 25802 2.75
lr 0.0007387466942153061
lr 0.0007387466942153061
Mask overlap ratio: 3.10
Mask Update
2024-12-25 09:51:34.922 | INFO     | __main__:evaluate_model:113 - Loaded validation dataset in 3.21 seconds
2024-12-25 09:51:34.923 | INFO     | __main__:evaluate_model:129 - Eval set prepared in 3.21 seconds
2024-12-25 09:52:10.956 | INFO     | __main__:main:576 - Eval loss at step 13000: 5.375699520111084,tokens_seen: 1299478764

loss at 26002 2.734375
Update steps:  70%|█████████████▎     | 14000/20000 [3:18:17<2:11:43,  1.32s/it]2024-12-25 10:05:44.277 | INFO     | __main__:main:564 - Performing evaluation at step 14000

loss at 26202 2.75

loss at 26402 2.78125

loss at 26602 2.6875

loss at 26802 2.703125
lr 0.0006719091226784151
lr 0.0006719091226784151
Mask overlap ratio: 3.10
Mask Update

loss at 27002 2.671875

loss at 27202 2.703125

loss at 27402 2.6875

loss at 27602 2.734375

loss at 27802 2.671875
lr 0.0006079958459120696
lr 0.0006079958459120696
Mask overlap ratio: 3.10
Mask Update
2024-12-25 10:05:48.110 | INFO     | __main__:evaluate_model:113 - Loaded validation dataset in 3.83 seconds
2024-12-25 10:05:48.111 | INFO     | __main__:evaluate_model:129 - Eval set prepared in 3.83 seconds
2024-12-25 10:06:23.490 | INFO     | __main__:main:576 - Eval loss at step 14000: 5.351290225982666,tokens_seen: 1399437184

loss at 28002 2.640625
Update steps:  75%|██████████████▎    | 15000/20000 [3:32:59<1:49:48,  1.32s/it]2024-12-25 10:20:24.420 | INFO     | __main__:main:564 - Performing evaluation at step 15000

loss at 28202 2.640625

loss at 28402 2.6875

loss at 28602 2.6875

loss at 28802 2.671875
lr 0.0005474434564037938
lr 0.0005474434564037938
Mask overlap ratio: 3.10
Mask Update

loss at 29002 2.71875

loss at 29202 2.671875

loss at 29402 2.671875

loss at 29602 2.671875

loss at 29802 2.703125
lr 0.0004906655883732444
lr 0.0004906655883732444
Mask overlap ratio: 3.10
Mask Update
2024-12-25 10:20:28.118 | INFO     | __main__:evaluate_model:113 - Loaded validation dataset in 3.70 seconds
2024-12-25 10:20:28.119 | INFO     | __main__:evaluate_model:129 - Eval set prepared in 3.70 seconds
2024-12-25 10:21:03.044 | INFO     | __main__:main:576 - Eval loss at step 15000: 5.310090065002441,tokens_seen: 1499499200

loss at 30002 2.703125
Update steps:  80%|███████████████▏   | 16000/20000 [3:47:09<1:27:58,  1.32s/it]2024-12-25 10:34:36.036 | INFO     | __main__:main:564 - Performing evaluation at step 16000

loss at 30202 2.6875

loss at 30402 2.71875

loss at 30602 2.703125

loss at 30802 2.671875
lr 0.00043805009223107054
lr 0.00043805009223107054
Mask overlap ratio: 3.10
Mask Update

loss at 31002 2.65625

loss at 31202 2.6875

loss at 31402 2.65625

loss at 31602 2.625

loss at 31802 2.625
lr 0.00038995638516733477
lr 0.00038995638516733477
Mask overlap ratio: 3.10
Mask Update
2024-12-25 10:34:40.236 | INFO     | __main__:evaluate_model:113 - Loaded validation dataset in 4.20 seconds
2024-12-25 10:34:40.237 | INFO     | __main__:evaluate_model:129 - Eval set prepared in 4.20 seconds
2024-12-25 10:35:14.266 | INFO     | __main__:main:576 - Eval loss at step 16000: 5.280083894729614,tokens_seen: 1599578388

loss at 32002 2.671875
Update steps:  85%|████████████████▏  | 17000/20000 [4:01:20<1:05:53,  1.32s/it]2024-12-25 10:48:45.461 | INFO     | __main__:main:564 - Performing evaluation at step 17000

loss at 32202 2.640625

loss at 32402 2.671875

loss at 32602 2.703125

loss at 32802 2.671875
lr 0.00034671299596765996
lr 0.00034671299596765996
Mask overlap ratio: 3.10
Mask Update

loss at 33002 2.609375

loss at 33202 2.65625

loss at 33402 2.65625

loss at 33602 2.640625

loss at 33802 2.6875
lr 0.00030861532082849955
lr 0.00030861532082849955
Mask overlap ratio: 3.10
Mask Update
2024-12-25 10:48:49.678 | INFO     | __main__:evaluate_model:113 - Loaded validation dataset in 4.22 seconds
2024-12-25 10:48:49.680 | INFO     | __main__:evaluate_model:129 - Eval set prepared in 4.22 seconds
2024-12-25 10:49:25.432 | INFO     | __main__:main:576 - Eval loss at step 17000: 5.2558300495147705,tokens_seen: 1699525960

loss at 34002 2.640625
Update steps:  90%|██████████████████▉  | 18000/20000 [4:15:31<43:55,  1.32s/it]2024-12-25 11:02:58.826 | INFO     | __main__:main:564 - Performing evaluation at step 18000

loss at 34202 2.640625

loss at 34402 2.640625

loss at 34602 2.671875

loss at 34802 2.65625
lr 0.00027592360550158807
lr 0.00027592360550158807
Mask overlap ratio: 3.10
Mask Update

loss at 35002 2.609375

loss at 35202 2.65625

loss at 35402 2.640625

loss at 35602 2.640625

loss at 35802 2.640625
lr 0.0002488611675515719
lr 0.0002488611675515719
Mask overlap ratio: 3.10
Mask Update
2024-12-25 11:03:02.790 | INFO     | __main__:evaluate_model:113 - Loaded validation dataset in 3.96 seconds
2024-12-25 11:03:02.791 | INFO     | __main__:evaluate_model:129 - Eval set prepared in 3.97 seconds
2024-12-25 11:03:37.060 | INFO     | __main__:main:576 - Eval loss at step 18000: 5.241448879241943,tokens_seen: 1799556424

loss at 36002 2.625
Update steps:  95%|███████████████████▉ | 19000/20000 [4:29:43<21:56,  1.32s/it]2024-12-25 11:17:08.286 | INFO     | __main__:main:564 - Performing evaluation at step 19000

loss at 36202 2.625

loss at 36402 2.65625

loss at 36602 2.65625

loss at 36802 2.640625
lr 0.0002276128708706008
lr 0.0002276128708706008
Mask overlap ratio: 3.10
Mask Update

loss at 37002 2.625

loss at 37202 2.640625

loss at 37402 2.640625

loss at 37602 2.671875

loss at 37802 2.625
lr 0.00021232386287049496
lr 0.00021232386287049496
Mask overlap ratio: 3.10
Mask Update
2024-12-25 11:17:12.513 | INFO     | __main__:evaluate_model:113 - Loaded validation dataset in 4.23 seconds
2024-12-25 11:17:12.515 | INFO     | __main__:evaluate_model:129 - Eval set prepared in 4.23 seconds
2024-12-25 11:17:47.215 | INFO     | __main__:main:576 - Eval loss at step 19000: 5.233519792556763,tokens_seen: 1899562530

loss at 38002 2.65625
Update steps: 100%|█████████████████████| 20000/20000 [4:43:53<00:00,  1.32s/it]2024-12-25 11:31:20.445 | INFO     | __main__:main:564 - Performing evaluation at step 20000

loss at 38202 2.65625

loss at 38402 2.640625

loss at 38602 2.625

loss at 38802 2.625
lr 0.00020309858297874538
lr 0.00020309858297874538
Mask overlap ratio: 3.10
Mask Update

loss at 39002 2.65625

loss at 39202 2.609375

loss at 39402 2.640625

loss at 39602 2.65625

loss at 39802 2.640625
lr 0.00020000004921132342
lr 0.00020000004921132342
Mask overlap ratio: 3.10
Mask Update
2024-12-25 11:31:24.461 | INFO     | __main__:evaluate_model:113 - Loaded validation dataset in 4.02 seconds
2024-12-25 11:31:24.463 | INFO     | __main__:evaluate_model:129 - Eval set prepared in 4.02 seconds
2024-12-25 11:31:59.125 | INFO     | __main__:main:576 - Eval loss at step 20000: 5.225746154785156,tokens_seen: 1999649470

loss at 40002 2.640625
Update steps: 20001it [4:44:35, 13.38s/it]                                      2024-12-25 11:31:59.907 | INFO     | __main__:main:493 - Reached max number of update steps (f20000). Stopping training.
Rank 0 stopping training.
2024-12-25 11:32:00.319 | INFO     | __main__:main:603 - Training finished
Update steps: 20001it [4:44:35,  1.17it/s]
2024-12-25 11:32:00.320 | INFO     | __main__:main:608 - Saving model and optimizer to /scratch-shared/HTJ2/checkpoints/new0_9144442/model_20001, update step 20001
2024-12-25 11:32:02.308 | INFO     | __main__:main:635 - Running final evaluation
2024-12-25 11:32:08.422 | INFO     | __main__:evaluate_model:113 - Loaded validation dataset in 5.90 seconds
2024-12-25 11:32:08.424 | INFO     | __main__:evaluate_model:129 - Eval set prepared in 5.90 seconds
2024-12-25 11:32:42.500 | INFO     | __main__:main:653 - Final eval loss: 5.225746154785156
2024-12-25 11:32:42.501 | INFO     | __main__:main:655 - Script finished successfully
Rank 0 finished successfully
perplexity 185.9999034794087
